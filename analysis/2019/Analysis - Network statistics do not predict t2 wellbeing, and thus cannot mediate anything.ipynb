{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network statistics do not predict t2 wellbeing outright, and thus cannot mediate anything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Configuration\" data-toc-modified-id=\"Configuration-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Configuration</a></span></li><li><span><a href=\"#Import-and-load\" data-toc-modified-id=\"Import-and-load-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Import and load</a></span></li><li><span><a href=\"#Run-the-base-model-predicting-t2-from-t1-wellbeing\" data-toc-modified-id=\"Run-the-base-model-predicting-t2-from-t1-wellbeing-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Run the base model predicting t2 from t1 wellbeing</a></span></li><li><span><a href=\"#Can-any-network-variable-predict-t2-wellbeing-outright?\" data-toc-modified-id=\"Can-any-network-variable-predict-t2-wellbeing-outright?-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Can any network variable predict t2 wellbeing outright?</a></span></li><li><span><a href=\"#OK,-what-about-everything?-(All-continuous-variables)\" data-toc-modified-id=\"OK,-what-about-everything?-(All-continuous-variables)-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>OK, what about <em>everything</em>? (All continuous variables)</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = '../data/2019–2020/postprocessed/df_Rcleaned_train.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: carData\n",
      "\n",
      "── \u001b[1mAttaching packages\u001b[22m ─────────────────────────────────────── tidyverse 1.3.0 ──\n",
      "\n",
      "\u001b[32m✔\u001b[39m \u001b[34mggplot2\u001b[39m 3.2.1     \u001b[32m✔\u001b[39m \u001b[34mpurrr  \u001b[39m 0.3.3\n",
      "\u001b[32m✔\u001b[39m \u001b[34mtibble \u001b[39m 2.1.3     \u001b[32m✔\u001b[39m \u001b[34mdplyr  \u001b[39m 0.8.3\n",
      "\u001b[32m✔\u001b[39m \u001b[34mtidyr  \u001b[39m 1.0.0     \u001b[32m✔\u001b[39m \u001b[34mstringr\u001b[39m 1.4.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mreadr  \u001b[39m 1.3.1     \u001b[32m✔\u001b[39m \u001b[34mforcats\u001b[39m 0.4.0\n",
      "\n",
      "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mrecode()\u001b[39m masks \u001b[34mcar\u001b[39m::recode()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mpurrr\u001b[39m::\u001b[32msome()\u001b[39m   masks \u001b[34mcar\u001b[39m::some()\n",
      "\n",
      "Loading required package: lattice\n",
      "\n",
      "Registered S3 methods overwritten by 'lme4':\n",
      "  method                          from\n",
      "  cooks.distance.influence.merMod car \n",
      "  influence.merMod                car \n",
      "  dfbeta.influence.merMod         car \n",
      "  dfbetas.influence.merMod        car \n",
      "\n",
      "\n",
      "Attaching package: ‘mice’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:tidyr’:\n",
      "\n",
      "    complete\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    cbind, rbind\n",
      "\n",
      "\n",
      "\n",
      "Attaching package: ‘nlme’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:dplyr’:\n",
      "\n",
      "    collapse\n",
      "\n",
      "\n",
      "Loading required package: Matrix\n",
      "\n",
      "\n",
      "Attaching package: ‘Matrix’\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:tidyr’:\n",
      "\n",
      "    expand, pack, unpack\n",
      "\n",
      "\n",
      "\n",
      "Attaching package: ‘lme4’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:nlme’:\n",
      "\n",
      "    lmList\n",
      "\n",
      "\n",
      "\n",
      "Attaching package: ‘lmerTest’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:lme4’:\n",
      "\n",
      "    lmer\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:stats’:\n",
      "\n",
      "    step\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(car)\n",
    "library(tidyverse)\n",
    "library(hexbin)\n",
    "library(mice)\n",
    "library(nlme)\n",
    "library(lme4)\n",
    "library(lmerTest)\n",
    "\n",
    "# Display more data in the Jupyter notebook\n",
    "options(repr.matrix.max.cols=500, repr.matrix.max.rows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>560</li>\n",
       "\t<li>59</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 560\n",
       "\\item 59\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 560\n",
       "2. 59\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 560  59"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.frame: 6 × 59</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>gender</th><th scope=col>race</th><th scope=col>dorm</th><th scope=col>life_satisfaction_t1</th><th scope=col>empathy</th><th scope=col>loneliness_t1</th><th scope=col>stress_t1</th><th scope=col>BFI_E</th><th scope=col>BFI_A</th><th scope=col>BFI_C</th><th scope=col>BFI_N</th><th scope=col>BFI_O</th><th scope=col>intl_student</th><th scope=col>family_income</th><th scope=col>life_satisfaction_t2</th><th scope=col>loneliness_t2</th><th scope=col>stress_t2</th><th scope=col>parent_education_highest</th><th scope=col>wellbeing_composite_t1</th><th scope=col>wellbeing_composite_t2</th><th scope=col>degree_in_UNION</th><th scope=col>degree_out_UNION</th><th scope=col>empathy_UNION</th><th scope=col>degree_in_INTIMATE</th><th scope=col>degree_out_INTIMATE</th><th scope=col>empathy_INTIMATE</th><th scope=col>degree_in_ACQUAINTANCE</th><th scope=col>degree_out_ACQUAINTANCE</th><th scope=col>empathy_ACQUAINTANCE</th><th scope=col>degree_in_CloseFrds</th><th scope=col>degree_out_CloseFrds</th><th scope=col>empathy_CloseFrds</th><th scope=col>degree_in_NegEmoSupp</th><th scope=col>degree_out_NegEmoSupp</th><th scope=col>empathy_NegEmoSupp</th><th scope=col>degree_in_PosEmoSupp</th><th scope=col>degree_out_PosEmoSupp</th><th scope=col>empathy_PosEmoSupp</th><th scope=col>degree_in_Responsive</th><th scope=col>degree_out_Responsive</th><th scope=col>empathy_Responsive</th><th scope=col>degree_in_EmpSupp</th><th scope=col>degree_out_EmpSupp</th><th scope=col>empathy_EmpSupp</th><th scope=col>degree_in_PosAff</th><th scope=col>degree_out_PosAff</th><th scope=col>empathy_PosAff</th><th scope=col>degree_in_NegAff</th><th scope=col>degree_out_NegAff</th><th scope=col>empathy_NegAff</th><th scope=col>degree_in_Gossip</th><th scope=col>degree_out_Gossip</th><th scope=col>empathy_Gossip</th><th scope=col>degree_in_Liked</th><th scope=col>degree_out_Liked</th><th scope=col>empathy_Liked</th><th scope=col>degree_in_StudyWith</th><th scope=col>degree_out_StudyWith</th><th scope=col>empathy_StudyWith</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>male  </td><td>south_asian   </td><td>Rinconada</td><td>4.833333</td><td>6.375</td><td>1.333333</td><td>2.5</td><td>6.5</td><td>6.0</td><td>5.5</td><td>3.5</td><td>6.5</td><td>0</td><td> 70000</td><td>6.166667</td><td>2.000000</td><td>1.5</td><td>5</td><td>-0.05862651</td><td> 0.83993983</td><td>3</td><td>0</td><td>      NA</td><td>1</td><td>0</td><td>     NA</td><td>1</td><td>0</td><td>   NA</td><td>1</td><td>0</td><td>     NA</td><td>0</td><td>0</td><td>      NA</td><td>2</td><td>0</td><td>    NA</td><td>0</td><td>0</td><td>    NA</td><td>0</td><td>0</td><td>    NA</td><td>1</td><td>0</td><td>     NA</td><td>0</td><td>0</td><td> NA</td><td>1</td><td>0</td><td>      NA</td><td>1</td><td>0</td><td>   NA</td><td>1</td><td>0</td><td>    NA</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>female</td><td>other_or_mixed</td><td>Twain    </td><td>5.333333</td><td>6.500</td><td>2.000000</td><td>2.5</td><td>5.5</td><td>6.5</td><td>7.0</td><td>3.0</td><td>6.5</td><td>0</td><td>210000</td><td>5.166667</td><td>2.000000</td><td>3.0</td><td>5</td><td>-0.44690440</td><td>-0.66253028</td><td>6</td><td>4</td><td>6.437500</td><td>4</td><td>4</td><td>6.43750</td><td>0</td><td>1</td><td>6.125</td><td>4</td><td>4</td><td>6.43750</td><td>4</td><td>3</td><td>6.541667</td><td>3</td><td>4</td><td>6.4375</td><td>2</td><td>2</td><td>6.5625</td><td>1</td><td>2</td><td>6.5625</td><td>3</td><td>4</td><td>6.43750</td><td>1</td><td>1</td><td>6.5</td><td>4</td><td>3</td><td>6.541667</td><td>1</td><td>2</td><td>6.000</td><td>2</td><td>1</td><td>6.6250</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>female</td><td>east_asian    </td><td>Loro     </td><td>5.500000</td><td>6.250</td><td>1.333333</td><td>2.0</td><td>4.5</td><td>6.5</td><td>6.5</td><td>1.5</td><td>6.5</td><td>0</td><td>130000</td><td>5.500000</td><td>2.333333</td><td>3.0</td><td>6</td><td> 0.61446613</td><td>-0.79801599</td><td>1</td><td>0</td><td>      NA</td><td>1</td><td>0</td><td>     NA</td><td>1</td><td>0</td><td>   NA</td><td>1</td><td>0</td><td>     NA</td><td>0</td><td>0</td><td>      NA</td><td>1</td><td>0</td><td>    NA</td><td>1</td><td>0</td><td>    NA</td><td>0</td><td>0</td><td>    NA</td><td>0</td><td>0</td><td>     NA</td><td>0</td><td>1</td><td>5.0</td><td>0</td><td>0</td><td>      NA</td><td>0</td><td>0</td><td>   NA</td><td>0</td><td>0</td><td>    NA</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>male  </td><td>east_asian    </td><td>Otero    </td><td>6.000000</td><td>5.000</td><td>2.000000</td><td>2.0</td><td>3.0</td><td>4.0</td><td>5.5</td><td>1.5</td><td>4.5</td><td>0</td><td> 90000</td><td>5.833333</td><td>2.000000</td><td>2.5</td><td>6</td><td> 0.22618824</td><td>-0.04126834</td><td>6</td><td>4</td><td>5.812500</td><td>3</td><td>1</td><td>5.62500</td><td>2</td><td>0</td><td>   NA</td><td>2</td><td>1</td><td>5.62500</td><td>1</td><td>1</td><td>5.625000</td><td>1</td><td>1</td><td>6.6250</td><td>0</td><td>1</td><td>5.6250</td><td>0</td><td>1</td><td>4.8750</td><td>1</td><td>1</td><td>6.12500</td><td>0</td><td>0</td><td> NA</td><td>0</td><td>1</td><td>5.625000</td><td>0</td><td>1</td><td>6.000</td><td>3</td><td>1</td><td>6.6250</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>female</td><td>white         </td><td>Larkin   </td><td>6.666667</td><td>5.750</td><td>2.000000</td><td>2.0</td><td>4.5</td><td>5.0</td><td>5.0</td><td>2.0</td><td>4.5</td><td>0</td><td>170000</td><td>6.666667</td><td>2.000000</td><td>2.0</td><td>4</td><td> 0.48041399</td><td> 0.64021289</td><td>6</td><td>9</td><td>5.847222</td><td>4</td><td>5</td><td>6.02500</td><td>2</td><td>2</td><td>5.875</td><td>3</td><td>4</td><td>5.81250</td><td>2</td><td>3</td><td>6.125000</td><td>2</td><td>2</td><td>5.3750</td><td>2</td><td>2</td><td>6.3125</td><td>1</td><td>4</td><td>6.1875</td><td>2</td><td>1</td><td>6.87500</td><td>1</td><td>0</td><td> NA</td><td>2</td><td>3</td><td>5.750000</td><td>1</td><td>1</td><td>5.125</td><td>4</td><td>2</td><td>6.1875</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>female</td><td>white         </td><td>Larkin   </td><td>5.166667</td><td>6.750</td><td>2.000000</td><td>2.0</td><td>6.5</td><td>5.0</td><td>6.0</td><td>2.0</td><td>4.5</td><td>0</td><td>110000</td><td>6.833333</td><td>1.666667</td><td>2.0</td><td>5</td><td>-0.09159395</td><td> 0.95635646</td><td>4</td><td>5</td><td>5.550000</td><td>4</td><td>4</td><td>5.40625</td><td>1</td><td>0</td><td>   NA</td><td>3</td><td>4</td><td>5.40625</td><td>3</td><td>4</td><td>5.406250</td><td>3</td><td>2</td><td>6.0625</td><td>2</td><td>2</td><td>5.7500</td><td>3</td><td>1</td><td>6.0000</td><td>2</td><td>4</td><td>5.78125</td><td>0</td><td>0</td><td> NA</td><td>3</td><td>4</td><td>5.406250</td><td>4</td><td>1</td><td>6.000</td><td>3</td><td>4</td><td>5.4375</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 59\n",
       "\\begin{tabular}{r|lllllllllllllllllllllllllllllllllllllllllllllllllllllllllll}\n",
       "  & gender & race & dorm & life\\_satisfaction\\_t1 & empathy & loneliness\\_t1 & stress\\_t1 & BFI\\_E & BFI\\_A & BFI\\_C & BFI\\_N & BFI\\_O & intl\\_student & family\\_income & life\\_satisfaction\\_t2 & loneliness\\_t2 & stress\\_t2 & parent\\_education\\_highest & wellbeing\\_composite\\_t1 & wellbeing\\_composite\\_t2 & degree\\_in\\_UNION & degree\\_out\\_UNION & empathy\\_UNION & degree\\_in\\_INTIMATE & degree\\_out\\_INTIMATE & empathy\\_INTIMATE & degree\\_in\\_ACQUAINTANCE & degree\\_out\\_ACQUAINTANCE & empathy\\_ACQUAINTANCE & degree\\_in\\_CloseFrds & degree\\_out\\_CloseFrds & empathy\\_CloseFrds & degree\\_in\\_NegEmoSupp & degree\\_out\\_NegEmoSupp & empathy\\_NegEmoSupp & degree\\_in\\_PosEmoSupp & degree\\_out\\_PosEmoSupp & empathy\\_PosEmoSupp & degree\\_in\\_Responsive & degree\\_out\\_Responsive & empathy\\_Responsive & degree\\_in\\_EmpSupp & degree\\_out\\_EmpSupp & empathy\\_EmpSupp & degree\\_in\\_PosAff & degree\\_out\\_PosAff & empathy\\_PosAff & degree\\_in\\_NegAff & degree\\_out\\_NegAff & empathy\\_NegAff & degree\\_in\\_Gossip & degree\\_out\\_Gossip & empathy\\_Gossip & degree\\_in\\_Liked & degree\\_out\\_Liked & empathy\\_Liked & degree\\_in\\_StudyWith & degree\\_out\\_StudyWith & empathy\\_StudyWith\\\\\n",
       "  & <fct> & <fct> & <fct> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <int> & <int> & <dbl> & <dbl> & <dbl> & <int> & <dbl> & <dbl> & <int> & <int> & <dbl> & <int> & <int> & <dbl> & <int> & <int> & <dbl> & <int> & <int> & <dbl> & <int> & <int> & <dbl> & <int> & <int> & <dbl> & <int> & <int> & <dbl> & <int> & <int> & <dbl> & <int> & <int> & <dbl> & <int> & <int> & <dbl> & <int> & <int> & <dbl> & <int> & <int> & <dbl> & <int> & <int> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & male   & south\\_asian    & Rinconada & 4.833333 & 6.375 & 1.333333 & 2.5 & 6.5 & 6.0 & 5.5 & 3.5 & 6.5 & 0 &  70000 & 6.166667 & 2.000000 & 1.5 & 5 & -0.05862651 &  0.83993983 & 3 & 0 &       NA & 1 & 0 &      NA & 1 & 0 &    NA & 1 & 0 &      NA & 0 & 0 &       NA & 2 & 0 &     NA & 0 & 0 &     NA & 0 & 0 &     NA & 1 & 0 &      NA & 0 & 0 &  NA & 1 & 0 &       NA & 1 & 0 &    NA & 1 & 0 &     NA\\\\\n",
       "\t2 & female & other\\_or\\_mixed & Twain     & 5.333333 & 6.500 & 2.000000 & 2.5 & 5.5 & 6.5 & 7.0 & 3.0 & 6.5 & 0 & 210000 & 5.166667 & 2.000000 & 3.0 & 5 & -0.44690440 & -0.66253028 & 6 & 4 & 6.437500 & 4 & 4 & 6.43750 & 0 & 1 & 6.125 & 4 & 4 & 6.43750 & 4 & 3 & 6.541667 & 3 & 4 & 6.4375 & 2 & 2 & 6.5625 & 1 & 2 & 6.5625 & 3 & 4 & 6.43750 & 1 & 1 & 6.5 & 4 & 3 & 6.541667 & 1 & 2 & 6.000 & 2 & 1 & 6.6250\\\\\n",
       "\t3 & female & east\\_asian     & Loro      & 5.500000 & 6.250 & 1.333333 & 2.0 & 4.5 & 6.5 & 6.5 & 1.5 & 6.5 & 0 & 130000 & 5.500000 & 2.333333 & 3.0 & 6 &  0.61446613 & -0.79801599 & 1 & 0 &       NA & 1 & 0 &      NA & 1 & 0 &    NA & 1 & 0 &      NA & 0 & 0 &       NA & 1 & 0 &     NA & 1 & 0 &     NA & 0 & 0 &     NA & 0 & 0 &      NA & 0 & 1 & 5.0 & 0 & 0 &       NA & 0 & 0 &    NA & 0 & 0 &     NA\\\\\n",
       "\t4 & male   & east\\_asian     & Otero     & 6.000000 & 5.000 & 2.000000 & 2.0 & 3.0 & 4.0 & 5.5 & 1.5 & 4.5 & 0 &  90000 & 5.833333 & 2.000000 & 2.5 & 6 &  0.22618824 & -0.04126834 & 6 & 4 & 5.812500 & 3 & 1 & 5.62500 & 2 & 0 &    NA & 2 & 1 & 5.62500 & 1 & 1 & 5.625000 & 1 & 1 & 6.6250 & 0 & 1 & 5.6250 & 0 & 1 & 4.8750 & 1 & 1 & 6.12500 & 0 & 0 &  NA & 0 & 1 & 5.625000 & 0 & 1 & 6.000 & 3 & 1 & 6.6250\\\\\n",
       "\t5 & female & white          & Larkin    & 6.666667 & 5.750 & 2.000000 & 2.0 & 4.5 & 5.0 & 5.0 & 2.0 & 4.5 & 0 & 170000 & 6.666667 & 2.000000 & 2.0 & 4 &  0.48041399 &  0.64021289 & 6 & 9 & 5.847222 & 4 & 5 & 6.02500 & 2 & 2 & 5.875 & 3 & 4 & 5.81250 & 2 & 3 & 6.125000 & 2 & 2 & 5.3750 & 2 & 2 & 6.3125 & 1 & 4 & 6.1875 & 2 & 1 & 6.87500 & 1 & 0 &  NA & 2 & 3 & 5.750000 & 1 & 1 & 5.125 & 4 & 2 & 6.1875\\\\\n",
       "\t6 & female & white          & Larkin    & 5.166667 & 6.750 & 2.000000 & 2.0 & 6.5 & 5.0 & 6.0 & 2.0 & 4.5 & 0 & 110000 & 6.833333 & 1.666667 & 2.0 & 5 & -0.09159395 &  0.95635646 & 4 & 5 & 5.550000 & 4 & 4 & 5.40625 & 1 & 0 &    NA & 3 & 4 & 5.40625 & 3 & 4 & 5.406250 & 3 & 2 & 6.0625 & 2 & 2 & 5.7500 & 3 & 1 & 6.0000 & 2 & 4 & 5.78125 & 0 & 0 &  NA & 3 & 4 & 5.406250 & 4 & 1 & 6.000 & 3 & 4 & 5.4375\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 59\n",
       "\n",
       "| <!--/--> | gender &lt;fct&gt; | race &lt;fct&gt; | dorm &lt;fct&gt; | life_satisfaction_t1 &lt;dbl&gt; | empathy &lt;dbl&gt; | loneliness_t1 &lt;dbl&gt; | stress_t1 &lt;dbl&gt; | BFI_E &lt;dbl&gt; | BFI_A &lt;dbl&gt; | BFI_C &lt;dbl&gt; | BFI_N &lt;dbl&gt; | BFI_O &lt;dbl&gt; | intl_student &lt;int&gt; | family_income &lt;int&gt; | life_satisfaction_t2 &lt;dbl&gt; | loneliness_t2 &lt;dbl&gt; | stress_t2 &lt;dbl&gt; | parent_education_highest &lt;int&gt; | wellbeing_composite_t1 &lt;dbl&gt; | wellbeing_composite_t2 &lt;dbl&gt; | degree_in_UNION &lt;int&gt; | degree_out_UNION &lt;int&gt; | empathy_UNION &lt;dbl&gt; | degree_in_INTIMATE &lt;int&gt; | degree_out_INTIMATE &lt;int&gt; | empathy_INTIMATE &lt;dbl&gt; | degree_in_ACQUAINTANCE &lt;int&gt; | degree_out_ACQUAINTANCE &lt;int&gt; | empathy_ACQUAINTANCE &lt;dbl&gt; | degree_in_CloseFrds &lt;int&gt; | degree_out_CloseFrds &lt;int&gt; | empathy_CloseFrds &lt;dbl&gt; | degree_in_NegEmoSupp &lt;int&gt; | degree_out_NegEmoSupp &lt;int&gt; | empathy_NegEmoSupp &lt;dbl&gt; | degree_in_PosEmoSupp &lt;int&gt; | degree_out_PosEmoSupp &lt;int&gt; | empathy_PosEmoSupp &lt;dbl&gt; | degree_in_Responsive &lt;int&gt; | degree_out_Responsive &lt;int&gt; | empathy_Responsive &lt;dbl&gt; | degree_in_EmpSupp &lt;int&gt; | degree_out_EmpSupp &lt;int&gt; | empathy_EmpSupp &lt;dbl&gt; | degree_in_PosAff &lt;int&gt; | degree_out_PosAff &lt;int&gt; | empathy_PosAff &lt;dbl&gt; | degree_in_NegAff &lt;int&gt; | degree_out_NegAff &lt;int&gt; | empathy_NegAff &lt;dbl&gt; | degree_in_Gossip &lt;int&gt; | degree_out_Gossip &lt;int&gt; | empathy_Gossip &lt;dbl&gt; | degree_in_Liked &lt;int&gt; | degree_out_Liked &lt;int&gt; | empathy_Liked &lt;dbl&gt; | degree_in_StudyWith &lt;int&gt; | degree_out_StudyWith &lt;int&gt; | empathy_StudyWith &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1 | male   | south_asian    | Rinconada | 4.833333 | 6.375 | 1.333333 | 2.5 | 6.5 | 6.0 | 5.5 | 3.5 | 6.5 | 0 |  70000 | 6.166667 | 2.000000 | 1.5 | 5 | -0.05862651 |  0.83993983 | 3 | 0 |       NA | 1 | 0 |      NA | 1 | 0 |    NA | 1 | 0 |      NA | 0 | 0 |       NA | 2 | 0 |     NA | 0 | 0 |     NA | 0 | 0 |     NA | 1 | 0 |      NA | 0 | 0 |  NA | 1 | 0 |       NA | 1 | 0 |    NA | 1 | 0 |     NA |\n",
       "| 2 | female | other_or_mixed | Twain     | 5.333333 | 6.500 | 2.000000 | 2.5 | 5.5 | 6.5 | 7.0 | 3.0 | 6.5 | 0 | 210000 | 5.166667 | 2.000000 | 3.0 | 5 | -0.44690440 | -0.66253028 | 6 | 4 | 6.437500 | 4 | 4 | 6.43750 | 0 | 1 | 6.125 | 4 | 4 | 6.43750 | 4 | 3 | 6.541667 | 3 | 4 | 6.4375 | 2 | 2 | 6.5625 | 1 | 2 | 6.5625 | 3 | 4 | 6.43750 | 1 | 1 | 6.5 | 4 | 3 | 6.541667 | 1 | 2 | 6.000 | 2 | 1 | 6.6250 |\n",
       "| 3 | female | east_asian     | Loro      | 5.500000 | 6.250 | 1.333333 | 2.0 | 4.5 | 6.5 | 6.5 | 1.5 | 6.5 | 0 | 130000 | 5.500000 | 2.333333 | 3.0 | 6 |  0.61446613 | -0.79801599 | 1 | 0 |       NA | 1 | 0 |      NA | 1 | 0 |    NA | 1 | 0 |      NA | 0 | 0 |       NA | 1 | 0 |     NA | 1 | 0 |     NA | 0 | 0 |     NA | 0 | 0 |      NA | 0 | 1 | 5.0 | 0 | 0 |       NA | 0 | 0 |    NA | 0 | 0 |     NA |\n",
       "| 4 | male   | east_asian     | Otero     | 6.000000 | 5.000 | 2.000000 | 2.0 | 3.0 | 4.0 | 5.5 | 1.5 | 4.5 | 0 |  90000 | 5.833333 | 2.000000 | 2.5 | 6 |  0.22618824 | -0.04126834 | 6 | 4 | 5.812500 | 3 | 1 | 5.62500 | 2 | 0 |    NA | 2 | 1 | 5.62500 | 1 | 1 | 5.625000 | 1 | 1 | 6.6250 | 0 | 1 | 5.6250 | 0 | 1 | 4.8750 | 1 | 1 | 6.12500 | 0 | 0 |  NA | 0 | 1 | 5.625000 | 0 | 1 | 6.000 | 3 | 1 | 6.6250 |\n",
       "| 5 | female | white          | Larkin    | 6.666667 | 5.750 | 2.000000 | 2.0 | 4.5 | 5.0 | 5.0 | 2.0 | 4.5 | 0 | 170000 | 6.666667 | 2.000000 | 2.0 | 4 |  0.48041399 |  0.64021289 | 6 | 9 | 5.847222 | 4 | 5 | 6.02500 | 2 | 2 | 5.875 | 3 | 4 | 5.81250 | 2 | 3 | 6.125000 | 2 | 2 | 5.3750 | 2 | 2 | 6.3125 | 1 | 4 | 6.1875 | 2 | 1 | 6.87500 | 1 | 0 |  NA | 2 | 3 | 5.750000 | 1 | 1 | 5.125 | 4 | 2 | 6.1875 |\n",
       "| 6 | female | white          | Larkin    | 5.166667 | 6.750 | 2.000000 | 2.0 | 6.5 | 5.0 | 6.0 | 2.0 | 4.5 | 0 | 110000 | 6.833333 | 1.666667 | 2.0 | 5 | -0.09159395 |  0.95635646 | 4 | 5 | 5.550000 | 4 | 4 | 5.40625 | 1 | 0 |    NA | 3 | 4 | 5.40625 | 3 | 4 | 5.406250 | 3 | 2 | 6.0625 | 2 | 2 | 5.7500 | 3 | 1 | 6.0000 | 2 | 4 | 5.78125 | 0 | 0 |  NA | 3 | 4 | 5.406250 | 4 | 1 | 6.000 | 3 | 4 | 5.4375 |\n",
       "\n"
      ],
      "text/plain": [
       "  gender race           dorm      life_satisfaction_t1 empathy loneliness_t1\n",
       "1 male   south_asian    Rinconada 4.833333             6.375   1.333333     \n",
       "2 female other_or_mixed Twain     5.333333             6.500   2.000000     \n",
       "3 female east_asian     Loro      5.500000             6.250   1.333333     \n",
       "4 male   east_asian     Otero     6.000000             5.000   2.000000     \n",
       "5 female white          Larkin    6.666667             5.750   2.000000     \n",
       "6 female white          Larkin    5.166667             6.750   2.000000     \n",
       "  stress_t1 BFI_E BFI_A BFI_C BFI_N BFI_O intl_student family_income\n",
       "1 2.5       6.5   6.0   5.5   3.5   6.5   0             70000       \n",
       "2 2.5       5.5   6.5   7.0   3.0   6.5   0            210000       \n",
       "3 2.0       4.5   6.5   6.5   1.5   6.5   0            130000       \n",
       "4 2.0       3.0   4.0   5.5   1.5   4.5   0             90000       \n",
       "5 2.0       4.5   5.0   5.0   2.0   4.5   0            170000       \n",
       "6 2.0       6.5   5.0   6.0   2.0   4.5   0            110000       \n",
       "  life_satisfaction_t2 loneliness_t2 stress_t2 parent_education_highest\n",
       "1 6.166667             2.000000      1.5       5                       \n",
       "2 5.166667             2.000000      3.0       5                       \n",
       "3 5.500000             2.333333      3.0       6                       \n",
       "4 5.833333             2.000000      2.5       6                       \n",
       "5 6.666667             2.000000      2.0       4                       \n",
       "6 6.833333             1.666667      2.0       5                       \n",
       "  wellbeing_composite_t1 wellbeing_composite_t2 degree_in_UNION\n",
       "1 -0.05862651             0.83993983            3              \n",
       "2 -0.44690440            -0.66253028            6              \n",
       "3  0.61446613            -0.79801599            1              \n",
       "4  0.22618824            -0.04126834            6              \n",
       "5  0.48041399             0.64021289            6              \n",
       "6 -0.09159395             0.95635646            4              \n",
       "  degree_out_UNION empathy_UNION degree_in_INTIMATE degree_out_INTIMATE\n",
       "1 0                      NA      1                  0                  \n",
       "2 4                6.437500      4                  4                  \n",
       "3 0                      NA      1                  0                  \n",
       "4 4                5.812500      3                  1                  \n",
       "5 9                5.847222      4                  5                  \n",
       "6 5                5.550000      4                  4                  \n",
       "  empathy_INTIMATE degree_in_ACQUAINTANCE degree_out_ACQUAINTANCE\n",
       "1      NA          1                      0                      \n",
       "2 6.43750          0                      1                      \n",
       "3      NA          1                      0                      \n",
       "4 5.62500          2                      0                      \n",
       "5 6.02500          2                      2                      \n",
       "6 5.40625          1                      0                      \n",
       "  empathy_ACQUAINTANCE degree_in_CloseFrds degree_out_CloseFrds\n",
       "1    NA                1                   0                   \n",
       "2 6.125                4                   4                   \n",
       "3    NA                1                   0                   \n",
       "4    NA                2                   1                   \n",
       "5 5.875                3                   4                   \n",
       "6    NA                3                   4                   \n",
       "  empathy_CloseFrds degree_in_NegEmoSupp degree_out_NegEmoSupp\n",
       "1      NA           0                    0                    \n",
       "2 6.43750           4                    3                    \n",
       "3      NA           0                    0                    \n",
       "4 5.62500           1                    1                    \n",
       "5 5.81250           2                    3                    \n",
       "6 5.40625           3                    4                    \n",
       "  empathy_NegEmoSupp degree_in_PosEmoSupp degree_out_PosEmoSupp\n",
       "1       NA           2                    0                    \n",
       "2 6.541667           3                    4                    \n",
       "3       NA           1                    0                    \n",
       "4 5.625000           1                    1                    \n",
       "5 6.125000           2                    2                    \n",
       "6 5.406250           3                    2                    \n",
       "  empathy_PosEmoSupp degree_in_Responsive degree_out_Responsive\n",
       "1     NA             0                    0                    \n",
       "2 6.4375             2                    2                    \n",
       "3     NA             1                    0                    \n",
       "4 6.6250             0                    1                    \n",
       "5 5.3750             2                    2                    \n",
       "6 6.0625             2                    2                    \n",
       "  empathy_Responsive degree_in_EmpSupp degree_out_EmpSupp empathy_EmpSupp\n",
       "1     NA             0                 0                      NA         \n",
       "2 6.5625             1                 2                  6.5625         \n",
       "3     NA             0                 0                      NA         \n",
       "4 5.6250             0                 1                  4.8750         \n",
       "5 6.3125             1                 4                  6.1875         \n",
       "6 5.7500             3                 1                  6.0000         \n",
       "  degree_in_PosAff degree_out_PosAff empathy_PosAff degree_in_NegAff\n",
       "1 1                0                      NA        0               \n",
       "2 3                4                 6.43750        1               \n",
       "3 0                0                      NA        0               \n",
       "4 1                1                 6.12500        0               \n",
       "5 2                1                 6.87500        1               \n",
       "6 2                4                 5.78125        0               \n",
       "  degree_out_NegAff empathy_NegAff degree_in_Gossip degree_out_Gossip\n",
       "1 0                  NA            1                0                \n",
       "2 1                 6.5            4                3                \n",
       "3 1                 5.0            0                0                \n",
       "4 0                  NA            0                1                \n",
       "5 0                  NA            2                3                \n",
       "6 0                  NA            3                4                \n",
       "  empathy_Gossip degree_in_Liked degree_out_Liked empathy_Liked\n",
       "1       NA       1               0                   NA        \n",
       "2 6.541667       1               2                6.000        \n",
       "3       NA       0               0                   NA        \n",
       "4 5.625000       0               1                6.000        \n",
       "5 5.750000       1               1                5.125        \n",
       "6 5.406250       4               1                6.000        \n",
       "  degree_in_StudyWith degree_out_StudyWith empathy_StudyWith\n",
       "1 1                   0                        NA           \n",
       "2 2                   1                    6.6250           \n",
       "3 0                   0                        NA           \n",
       "4 3                   1                    6.6250           \n",
       "5 4                   2                    6.1875           \n",
       "6 3                   4                    5.4375           "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = read.csv(DATA_FILE, na.strings=c(\"\", \" \", \"NA\"))\n",
    "df = df[,3:length(names(df))]\n",
    "dim(df)\n",
    "head(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the base model predicting t2 from t1 wellbeing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A matrix: 2 × 4 of type dbl</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>Estimate</th><th scope=col>Std. Error</th><th scope=col>t value</th><th scope=col>Pr(&gt;|t|)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>(Intercept)</th><td>-0.01387322</td><td>0.03136429</td><td>-0.4423255</td><td>6.584260e-01</td></tr>\n",
       "\t<tr><th scope=row>wellbeing_composite_t1</th><td> 0.66765669</td><td>0.03156521</td><td>21.1516612</td><td>2.864762e-73</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 2 × 4 of type dbl\n",
       "\\begin{tabular}{r|llll}\n",
       "  & Estimate & Std. Error & t value & Pr(>\\textbar{}t\\textbar{})\\\\\n",
       "\\hline\n",
       "\t(Intercept) & -0.01387322 & 0.03136429 & -0.4423255 & 6.584260e-01\\\\\n",
       "\twellbeing\\_composite\\_t1 &  0.66765669 & 0.03156521 & 21.1516612 & 2.864762e-73\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 2 × 4 of type dbl\n",
       "\n",
       "| <!--/--> | Estimate | Std. Error | t value | Pr(&gt;|t|) |\n",
       "|---|---|---|---|---|\n",
       "| (Intercept) | -0.01387322 | 0.03136429 | -0.4423255 | 6.584260e-01 |\n",
       "| wellbeing_composite_t1 |  0.66765669 | 0.03156521 | 21.1516612 | 2.864762e-73 |\n",
       "\n"
      ],
      "text/plain": [
       "                       Estimate    Std. Error t value    Pr(>|t|)    \n",
       "(Intercept)            -0.01387322 0.03136429 -0.4423255 6.584260e-01\n",
       "wellbeing_composite_t1  0.66765669 0.03156521 21.1516612 2.864762e-73"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = lm(wellbeing_composite_t2 ~ wellbeing_composite_t1, df)\n",
    "summary(base_model)$coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can any network variable predict t2 wellbeing outright?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.frame: 26 × 4</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>column</th><th scope=col>beta</th><th scope=col>p</th><th scope=col>p_adjusted</th></tr>\n",
       "\t<tr><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>degree_in_PosAff       </td><td> 0.074478432</td><td>0.003853911</td><td>0.08553443</td></tr>\n",
       "\t<tr><td>degree_in_Liked        </td><td> 0.058850126</td><td>0.006579572</td><td>0.08553443</td></tr>\n",
       "\t<tr><td>degree_in_INTIMATE     </td><td> 0.041807682</td><td>0.039158391</td><td>0.17735021</td></tr>\n",
       "\t<tr><td>degree_in_CloseFrds    </td><td> 0.050651489</td><td>0.022976539</td><td>0.17735021</td></tr>\n",
       "\t<tr><td>degree_out_Responsive  </td><td> 0.075830152</td><td>0.032280957</td><td>0.17735021</td></tr>\n",
       "\t<tr><td>degree_out_NegAff      </td><td>-0.130562621</td><td>0.047098481</td><td>0.17735021</td></tr>\n",
       "\t<tr><td>degree_in_StudyWith    </td><td> 0.059552474</td><td>0.047748133</td><td>0.17735021</td></tr>\n",
       "\t<tr><td>degree_in_UNION        </td><td> 0.025867299</td><td>0.080796715</td><td>0.21974433</td></tr>\n",
       "\t<tr><td>degree_in_NegEmoSupp   </td><td> 0.046411682</td><td>0.084517051</td><td>0.21974433</td></tr>\n",
       "\t<tr><td>degree_in_PosEmoSupp   </td><td> 0.051485511</td><td>0.078043720</td><td>0.21974433</td></tr>\n",
       "\t<tr><td>degree_in_Responsive   </td><td> 0.047592600</td><td>0.103617022</td><td>0.24491296</td></tr>\n",
       "\t<tr><td>degree_out_NegEmoSupp  </td><td> 0.048359197</td><td>0.138831920</td><td>0.29870722</td></tr>\n",
       "\t<tr><td>degree_out_PosAff      </td><td> 0.048643617</td><td>0.160842349</td><td>0.29870722</td></tr>\n",
       "\t<tr><td>degree_out_StudyWith   </td><td> 0.050311378</td><td>0.159670864</td><td>0.29870722</td></tr>\n",
       "\t<tr><td>degree_in_ACQUAINTANCE </td><td> 0.048247005</td><td>0.178731884</td><td>0.30980193</td></tr>\n",
       "\t<tr><td>degree_out_ACQUAINTANCE</td><td>-0.040399031</td><td>0.358903274</td><td>0.51841584</td></tr>\n",
       "\t<tr><td>degree_out_Gossip      </td><td> 0.033228772</td><td>0.346880179</td><td>0.51841584</td></tr>\n",
       "\t<tr><td>degree_out_Liked       </td><td> 0.038353163</td><td>0.332883941</td><td>0.51841584</td></tr>\n",
       "\t<tr><td>degree_out_INTIMATE    </td><td> 0.019748171</td><td>0.487533593</td><td>0.55313375</td></tr>\n",
       "\t<tr><td>degree_out_CloseFrds   </td><td> 0.021356555</td><td>0.489310624</td><td>0.55313375</td></tr>\n",
       "\t<tr><td>degree_out_PosEmoSupp  </td><td> 0.023975314</td><td>0.471673093</td><td>0.55313375</td></tr>\n",
       "\t<tr><td>degree_out_EmpSupp     </td><td> 0.034165983</td><td>0.414579055</td><td>0.55313375</td></tr>\n",
       "\t<tr><td>degree_in_NegAff       </td><td>-0.034222319</td><td>0.452782493</td><td>0.55313375</td></tr>\n",
       "\t<tr><td>degree_in_EmpSupp      </td><td>-0.015246958</td><td>0.601346714</td><td>0.62540058</td></tr>\n",
       "\t<tr><td>degree_in_Gossip       </td><td> 0.017117553</td><td>0.585016964</td><td>0.62540058</td></tr>\n",
       "\t<tr><td>degree_out_UNION       </td><td> 0.006154255</td><td>0.748330942</td><td>0.74833094</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 26 × 4\n",
       "\\begin{tabular}{llll}\n",
       " column & beta & p & p\\_adjusted\\\\\n",
       " <fct> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t degree\\_in\\_PosAff        &  0.074478432 & 0.003853911 & 0.08553443\\\\\n",
       "\t degree\\_in\\_Liked         &  0.058850126 & 0.006579572 & 0.08553443\\\\\n",
       "\t degree\\_in\\_INTIMATE      &  0.041807682 & 0.039158391 & 0.17735021\\\\\n",
       "\t degree\\_in\\_CloseFrds     &  0.050651489 & 0.022976539 & 0.17735021\\\\\n",
       "\t degree\\_out\\_Responsive   &  0.075830152 & 0.032280957 & 0.17735021\\\\\n",
       "\t degree\\_out\\_NegAff       & -0.130562621 & 0.047098481 & 0.17735021\\\\\n",
       "\t degree\\_in\\_StudyWith     &  0.059552474 & 0.047748133 & 0.17735021\\\\\n",
       "\t degree\\_in\\_UNION         &  0.025867299 & 0.080796715 & 0.21974433\\\\\n",
       "\t degree\\_in\\_NegEmoSupp    &  0.046411682 & 0.084517051 & 0.21974433\\\\\n",
       "\t degree\\_in\\_PosEmoSupp    &  0.051485511 & 0.078043720 & 0.21974433\\\\\n",
       "\t degree\\_in\\_Responsive    &  0.047592600 & 0.103617022 & 0.24491296\\\\\n",
       "\t degree\\_out\\_NegEmoSupp   &  0.048359197 & 0.138831920 & 0.29870722\\\\\n",
       "\t degree\\_out\\_PosAff       &  0.048643617 & 0.160842349 & 0.29870722\\\\\n",
       "\t degree\\_out\\_StudyWith    &  0.050311378 & 0.159670864 & 0.29870722\\\\\n",
       "\t degree\\_in\\_ACQUAINTANCE  &  0.048247005 & 0.178731884 & 0.30980193\\\\\n",
       "\t degree\\_out\\_ACQUAINTANCE & -0.040399031 & 0.358903274 & 0.51841584\\\\\n",
       "\t degree\\_out\\_Gossip       &  0.033228772 & 0.346880179 & 0.51841584\\\\\n",
       "\t degree\\_out\\_Liked        &  0.038353163 & 0.332883941 & 0.51841584\\\\\n",
       "\t degree\\_out\\_INTIMATE     &  0.019748171 & 0.487533593 & 0.55313375\\\\\n",
       "\t degree\\_out\\_CloseFrds    &  0.021356555 & 0.489310624 & 0.55313375\\\\\n",
       "\t degree\\_out\\_PosEmoSupp   &  0.023975314 & 0.471673093 & 0.55313375\\\\\n",
       "\t degree\\_out\\_EmpSupp      &  0.034165983 & 0.414579055 & 0.55313375\\\\\n",
       "\t degree\\_in\\_NegAff        & -0.034222319 & 0.452782493 & 0.55313375\\\\\n",
       "\t degree\\_in\\_EmpSupp       & -0.015246958 & 0.601346714 & 0.62540058\\\\\n",
       "\t degree\\_in\\_Gossip        &  0.017117553 & 0.585016964 & 0.62540058\\\\\n",
       "\t degree\\_out\\_UNION        &  0.006154255 & 0.748330942 & 0.74833094\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 26 × 4\n",
       "\n",
       "| column &lt;fct&gt; | beta &lt;dbl&gt; | p &lt;dbl&gt; | p_adjusted &lt;dbl&gt; |\n",
       "|---|---|---|---|\n",
       "| degree_in_PosAff        |  0.074478432 | 0.003853911 | 0.08553443 |\n",
       "| degree_in_Liked         |  0.058850126 | 0.006579572 | 0.08553443 |\n",
       "| degree_in_INTIMATE      |  0.041807682 | 0.039158391 | 0.17735021 |\n",
       "| degree_in_CloseFrds     |  0.050651489 | 0.022976539 | 0.17735021 |\n",
       "| degree_out_Responsive   |  0.075830152 | 0.032280957 | 0.17735021 |\n",
       "| degree_out_NegAff       | -0.130562621 | 0.047098481 | 0.17735021 |\n",
       "| degree_in_StudyWith     |  0.059552474 | 0.047748133 | 0.17735021 |\n",
       "| degree_in_UNION         |  0.025867299 | 0.080796715 | 0.21974433 |\n",
       "| degree_in_NegEmoSupp    |  0.046411682 | 0.084517051 | 0.21974433 |\n",
       "| degree_in_PosEmoSupp    |  0.051485511 | 0.078043720 | 0.21974433 |\n",
       "| degree_in_Responsive    |  0.047592600 | 0.103617022 | 0.24491296 |\n",
       "| degree_out_NegEmoSupp   |  0.048359197 | 0.138831920 | 0.29870722 |\n",
       "| degree_out_PosAff       |  0.048643617 | 0.160842349 | 0.29870722 |\n",
       "| degree_out_StudyWith    |  0.050311378 | 0.159670864 | 0.29870722 |\n",
       "| degree_in_ACQUAINTANCE  |  0.048247005 | 0.178731884 | 0.30980193 |\n",
       "| degree_out_ACQUAINTANCE | -0.040399031 | 0.358903274 | 0.51841584 |\n",
       "| degree_out_Gossip       |  0.033228772 | 0.346880179 | 0.51841584 |\n",
       "| degree_out_Liked        |  0.038353163 | 0.332883941 | 0.51841584 |\n",
       "| degree_out_INTIMATE     |  0.019748171 | 0.487533593 | 0.55313375 |\n",
       "| degree_out_CloseFrds    |  0.021356555 | 0.489310624 | 0.55313375 |\n",
       "| degree_out_PosEmoSupp   |  0.023975314 | 0.471673093 | 0.55313375 |\n",
       "| degree_out_EmpSupp      |  0.034165983 | 0.414579055 | 0.55313375 |\n",
       "| degree_in_NegAff        | -0.034222319 | 0.452782493 | 0.55313375 |\n",
       "| degree_in_EmpSupp       | -0.015246958 | 0.601346714 | 0.62540058 |\n",
       "| degree_in_Gossip        |  0.017117553 | 0.585016964 | 0.62540058 |\n",
       "| degree_out_UNION        |  0.006154255 | 0.748330942 | 0.74833094 |\n",
       "\n"
      ],
      "text/plain": [
       "   column                  beta         p           p_adjusted\n",
       "1  degree_in_PosAff         0.074478432 0.003853911 0.08553443\n",
       "2  degree_in_Liked          0.058850126 0.006579572 0.08553443\n",
       "3  degree_in_INTIMATE       0.041807682 0.039158391 0.17735021\n",
       "4  degree_in_CloseFrds      0.050651489 0.022976539 0.17735021\n",
       "5  degree_out_Responsive    0.075830152 0.032280957 0.17735021\n",
       "6  degree_out_NegAff       -0.130562621 0.047098481 0.17735021\n",
       "7  degree_in_StudyWith      0.059552474 0.047748133 0.17735021\n",
       "8  degree_in_UNION          0.025867299 0.080796715 0.21974433\n",
       "9  degree_in_NegEmoSupp     0.046411682 0.084517051 0.21974433\n",
       "10 degree_in_PosEmoSupp     0.051485511 0.078043720 0.21974433\n",
       "11 degree_in_Responsive     0.047592600 0.103617022 0.24491296\n",
       "12 degree_out_NegEmoSupp    0.048359197 0.138831920 0.29870722\n",
       "13 degree_out_PosAff        0.048643617 0.160842349 0.29870722\n",
       "14 degree_out_StudyWith     0.050311378 0.159670864 0.29870722\n",
       "15 degree_in_ACQUAINTANCE   0.048247005 0.178731884 0.30980193\n",
       "16 degree_out_ACQUAINTANCE -0.040399031 0.358903274 0.51841584\n",
       "17 degree_out_Gossip        0.033228772 0.346880179 0.51841584\n",
       "18 degree_out_Liked         0.038353163 0.332883941 0.51841584\n",
       "19 degree_out_INTIMATE      0.019748171 0.487533593 0.55313375\n",
       "20 degree_out_CloseFrds     0.021356555 0.489310624 0.55313375\n",
       "21 degree_out_PosEmoSupp    0.023975314 0.471673093 0.55313375\n",
       "22 degree_out_EmpSupp       0.034165983 0.414579055 0.55313375\n",
       "23 degree_in_NegAff        -0.034222319 0.452782493 0.55313375\n",
       "24 degree_in_EmpSupp       -0.015246958 0.601346714 0.62540058\n",
       "25 degree_in_Gossip         0.017117553 0.585016964 0.62540058\n",
       "26 degree_out_UNION         0.006154255 0.748330942 0.74833094"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "frame = NULL\n",
    "for (col in names(df)) {\n",
    "    if (startsWith(col, 'degree')) {\n",
    "        model = lm(as.formula(paste('wellbeing_composite_t2 ~', col)), df)\n",
    "        b = summary(model)$coefficients[2, 1]\n",
    "        p = summary(model)$coefficients[2, 4]\n",
    "        frame = rbind(frame, c(col, b, p))\n",
    "    }\n",
    "}\n",
    "frame = as.data.frame(frame)\n",
    "names(frame) = c('column', 'beta', 'p')\n",
    "frame$beta = as.numeric(as.character(frame$beta))\n",
    "frame$p = as.numeric(as.character(frame$p))\n",
    "frame$p_adjusted = p.adjust(frame$p, method=\"BH\")\n",
    "frame %>% arrange(p_adjusted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OK, what about *everything*? (All continuous variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.frame: 76 × 4</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>column</th><th scope=col>beta</th><th scope=col>p</th><th scope=col>p_adjusted</th></tr>\n",
       "\t<tr><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>wellbeing_composite_t1  </td><td> 6.676567e-01</td><td>2.864762e-73</td><td>2.177219e-71</td></tr>\n",
       "\t<tr><td>BFI_N                   </td><td>-3.162692e-01</td><td>1.195296e-25</td><td>4.542127e-24</td></tr>\n",
       "\t<tr><td>family_income           </td><td> 3.676776e-06</td><td>5.560889e-10</td><td>1.141938e-08</td></tr>\n",
       "\t<tr><td>parent_education_highest</td><td> 1.773958e-01</td><td>6.010200e-10</td><td>1.141938e-08</td></tr>\n",
       "\t<tr><td>BFI_E                   </td><td> 1.198556e-01</td><td>2.216161e-05</td><td>3.368565e-04</td></tr>\n",
       "\t<tr><td>BFI_C                   </td><td> 1.433248e-01</td><td>7.421972e-05</td><td>9.401165e-04</td></tr>\n",
       "\t<tr><td>genderother             </td><td>-1.495738e+00</td><td>7.373868e-04</td><td>8.005913e-03</td></tr>\n",
       "\t<tr><td>degree_in_PosAff        </td><td> 7.447843e-02</td><td>3.853911e-03</td><td>3.661215e-02</td></tr>\n",
       "\t<tr><td>degree_in_Liked         </td><td> 5.885013e-02</td><td>6.579572e-03</td><td>5.556083e-02</td></tr>\n",
       "\t<tr><td>gendermale              </td><td> 2.036963e-01</td><td>1.648248e-02</td><td>1.252669e-01</td></tr>\n",
       "\t<tr><td>degree_in_CloseFrds     </td><td> 5.065149e-02</td><td>2.297654e-02</td><td>1.587470e-01</td></tr>\n",
       "\t<tr><td>degree_out_Responsive   </td><td> 7.583015e-02</td><td>3.228096e-02</td><td>2.044461e-01</td></tr>\n",
       "\t<tr><td>racewhite               </td><td> 3.103139e-01</td><td>3.903892e-02</td><td>2.125741e-01</td></tr>\n",
       "\t<tr><td>degree_in_INTIMATE      </td><td> 4.180768e-02</td><td>3.915839e-02</td><td>2.125741e-01</td></tr>\n",
       "\t<tr><td>degree_out_NegAff       </td><td>-1.305626e-01</td><td>4.709848e-02</td><td>2.268036e-01</td></tr>\n",
       "\t<tr><td>degree_in_StudyWith     </td><td> 5.955247e-02</td><td>4.774813e-02</td><td>2.268036e-01</td></tr>\n",
       "\t<tr><td>dormFaisan              </td><td>-6.097550e-01</td><td>5.461906e-02</td><td>2.434837e-01</td></tr>\n",
       "\t<tr><td>empathy_Gossip          </td><td> 1.714183e-01</td><td>5.766719e-02</td><td>2.434837e-01</td></tr>\n",
       "\t<tr><td>dormSally Ride          </td><td> 5.111810e-01</td><td>6.616253e-02</td><td>2.646501e-01</td></tr>\n",
       "\t<tr><td>degree_in_UNION         </td><td> 2.586730e-02</td><td>8.079672e-02</td><td>2.919680e-01</td></tr>\n",
       "\t<tr><td>degree_in_NegEmoSupp    </td><td> 4.641168e-02</td><td>8.451705e-02</td><td>2.919680e-01</td></tr>\n",
       "\t<tr><td>degree_in_PosEmoSupp    </td><td> 5.148551e-02</td><td>7.804372e-02</td><td>2.919680e-01</td></tr>\n",
       "\t<tr><td>dormTwain               </td><td> 4.316860e-01</td><td>9.039257e-02</td><td>2.986885e-01</td></tr>\n",
       "\t<tr><td>dormOkada               </td><td>-5.350986e-01</td><td>9.801167e-02</td><td>3.032138e-01</td></tr>\n",
       "\t<tr><td>dormOtero               </td><td> 4.189891e-01</td><td>1.037310e-01</td><td>3.032138e-01</td></tr>\n",
       "\t<tr><td>degree_in_Responsive    </td><td> 4.759260e-02</td><td>1.036170e-01</td><td>3.032138e-01</td></tr>\n",
       "\t<tr><td>BFI_A                   </td><td> 6.052139e-02</td><td>1.136127e-01</td><td>3.197988e-01</td></tr>\n",
       "\t<tr><td>BFI_O                   </td><td> 6.243507e-02</td><td>1.456096e-01</td><td>3.688776e-01</td></tr>\n",
       "\t<tr><td>degree_out_NegEmoSupp   </td><td> 4.835920e-02</td><td>1.388319e-01</td><td>3.688776e-01</td></tr>\n",
       "\t<tr><td>empathy_Liked           </td><td>-1.443963e-01</td><td>1.440223e-01</td><td>3.688776e-01</td></tr>\n",
       "\t<tr><td>degree_out_PosAff       </td><td> 4.864362e-02</td><td>1.608423e-01</td><td>3.820006e-01</td></tr>\n",
       "\t<tr><td>degree_out_StudyWith    </td><td> 5.031138e-02</td><td>1.596709e-01</td><td>3.820006e-01</td></tr>\n",
       "\t<tr><td>degree_in_ACQUAINTANCE  </td><td> 4.824701e-02</td><td>1.787319e-01</td><td>4.116249e-01</td></tr>\n",
       "\t<tr><td>dormArroyo              </td><td> 2.894938e-01</td><td>2.816514e-01</td><td>6.295736e-01</td></tr>\n",
       "\t<tr><td>degree_out_Liked        </td><td> 3.835316e-02</td><td>3.328839e-01</td><td>7.228337e-01</td></tr>\n",
       "\t<tr><td>degree_out_Gossip       </td><td> 3.322877e-02</td><td>3.468802e-01</td><td>7.323026e-01</td></tr>\n",
       "\t<tr><td>degree_out_ACQUAINTANCE </td><td>-4.039903e-02</td><td>3.589033e-01</td><td>7.372067e-01</td></tr>\n",
       "\t<tr><td>dormNorcliffe           </td><td> 2.378226e-01</td><td>4.013319e-01</td><td>8.026638e-01</td></tr>\n",
       "\t<tr><td>degree_out_EmpSupp      </td><td> 3.416598e-02</td><td>4.145791e-01</td><td>8.078976e-01</td></tr>\n",
       "\t<tr><td>racesouth_asian         </td><td> 1.299660e-01</td><td>4.785940e-01</td><td>8.084262e-01</td></tr>\n",
       "\t<tr><td>degree_out_INTIMATE     </td><td> 1.974817e-02</td><td>4.875336e-01</td><td>8.084262e-01</td></tr>\n",
       "\t<tr><td>degree_out_CloseFrds    </td><td> 2.135656e-02</td><td>4.893106e-01</td><td>8.084262e-01</td></tr>\n",
       "\t<tr><td>degree_out_PosEmoSupp   </td><td> 2.397531e-02</td><td>4.716731e-01</td><td>8.084262e-01</td></tr>\n",
       "\t<tr><td>empathy_PosEmoSupp      </td><td> 6.562227e-02</td><td>4.638877e-01</td><td>8.084262e-01</td></tr>\n",
       "\t<tr><td>empathy_EmpSupp         </td><td> 7.151584e-02</td><td>4.441996e-01</td><td>8.084262e-01</td></tr>\n",
       "\t<tr><td>degree_in_NegAff        </td><td>-3.422232e-02</td><td>4.527825e-01</td><td>8.084262e-01</td></tr>\n",
       "\t<tr><td>racehispanic            </td><td>-1.140033e-01</td><td>5.166226e-01</td><td>8.179858e-01</td></tr>\n",
       "\t<tr><td>dormRinconada           </td><td> 1.763862e-01</td><td>5.116973e-01</td><td>8.179858e-01</td></tr>\n",
       "\t<tr><td>dormLarkin              </td><td> 1.366665e-01</td><td>5.703091e-01</td><td>8.668699e-01</td></tr>\n",
       "\t<tr><td>empathy_StudyWith       </td><td> 5.400922e-02</td><td>5.702951e-01</td><td>8.668699e-01</td></tr>\n",
       "\t<tr><td>degree_in_Gossip        </td><td> 1.711755e-02</td><td>5.850170e-01</td><td>8.717900e-01</td></tr>\n",
       "\t<tr><td>degree_in_EmpSupp       </td><td>-1.524696e-02</td><td>6.013467e-01</td><td>8.788914e-01</td></tr>\n",
       "\t<tr><td>empathy_UNION           </td><td>-4.744776e-02</td><td>6.388674e-01</td><td>8.827986e-01</td></tr>\n",
       "\t<tr><td>empathy_INTIMATE        </td><td>-4.585499e-02</td><td>6.340394e-01</td><td>8.827986e-01</td></tr>\n",
       "\t<tr><td>empathy_ACQUAINTANCE    </td><td>-5.332378e-02</td><td>6.198738e-01</td><td>8.827986e-01</td></tr>\n",
       "\t<tr><td>dormFroSoCo             </td><td>-1.123894e-01</td><td>6.544860e-01</td><td>8.882310e-01</td></tr>\n",
       "\t<tr><td>dormRoble               </td><td> 9.724519e-02</td><td>6.908443e-01</td><td>8.899011e-01</td></tr>\n",
       "\t<tr><td>intl_student            </td><td> 5.025582e-02</td><td>6.833949e-01</td><td>8.899011e-01</td></tr>\n",
       "\t<tr><td>empathy_CloseFrds       </td><td>-3.652359e-02</td><td>6.869275e-01</td><td>8.899011e-01</td></tr>\n",
       "\t<tr><td>raceeast_asian          </td><td> 4.914058e-02</td><td>7.562245e-01</td><td>9.122709e-01</td></tr>\n",
       "\t<tr><td>dormEucalipto           </td><td>-1.111507e-01</td><td>7.557527e-01</td><td>9.122709e-01</td></tr>\n",
       "\t<tr><td>dormJunipero            </td><td>-9.040046e-02</td><td>7.330049e-01</td><td>9.122709e-01</td></tr>\n",
       "\t<tr><td>degree_out_UNION        </td><td> 6.154255e-03</td><td>7.483309e-01</td><td>9.122709e-01</td></tr>\n",
       "\t<tr><td>dormZapata              </td><td> 1.060936e-01</td><td>7.820975e-01</td><td>9.257624e-01</td></tr>\n",
       "\t<tr><td>empathy_NegEmoSupp      </td><td> 2.394283e-02</td><td>7.917705e-01</td><td>9.257624e-01</td></tr>\n",
       "\t<tr><td>empathy                 </td><td> 1.404412e-02</td><td>8.225270e-01</td><td>9.393732e-01</td></tr>\n",
       "\t<tr><td>empathy_NegAff          </td><td> 3.061256e-02</td><td>8.281317e-01</td><td>9.393732e-01</td></tr>\n",
       "\t<tr><td>dormLoro                </td><td>-6.367529e-02</td><td>8.505471e-01</td><td>9.506115e-01</td></tr>\n",
       "\t<tr><td>empathy_Responsive      </td><td> 1.358870e-02</td><td>8.803754e-01</td><td>9.696889e-01</td></tr>\n",
       "\t<tr><td>raceother_or_mixed      </td><td> 1.917614e-02</td><td>9.014388e-01</td><td>9.768691e-01</td></tr>\n",
       "\t<tr><td>dormBurbank             </td><td> 1.970447e-02</td><td>9.383085e-01</td><td>9.768691e-01</td></tr>\n",
       "\t<tr><td>dormCedro               </td><td> 2.182167e-02</td><td>9.348168e-01</td><td>9.768691e-01</td></tr>\n",
       "\t<tr><td>dormMuwekma-Tah-Ruk     </td><td> 3.798578e-02</td><td>9.281985e-01</td><td>9.768691e-01</td></tr>\n",
       "\t<tr><td>dormGavilan             </td><td>-2.224323e-02</td><td>9.557231e-01</td><td>9.815534e-01</td></tr>\n",
       "\t<tr><td>empathy_PosAff          </td><td> 3.132939e-03</td><td>9.720067e-01</td><td>9.849668e-01</td></tr>\n",
       "\t<tr><td>dormUjamaa              </td><td>-2.871713e-03</td><td>9.921860e-01</td><td>9.921860e-01</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 76 × 4\n",
       "\\begin{tabular}{llll}\n",
       " column & beta & p & p\\_adjusted\\\\\n",
       " <fct> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t wellbeing\\_composite\\_t1   &  6.676567e-01 & 2.864762e-73 & 2.177219e-71\\\\\n",
       "\t BFI\\_N                    & -3.162692e-01 & 1.195296e-25 & 4.542127e-24\\\\\n",
       "\t family\\_income            &  3.676776e-06 & 5.560889e-10 & 1.141938e-08\\\\\n",
       "\t parent\\_education\\_highest &  1.773958e-01 & 6.010200e-10 & 1.141938e-08\\\\\n",
       "\t BFI\\_E                    &  1.198556e-01 & 2.216161e-05 & 3.368565e-04\\\\\n",
       "\t BFI\\_C                    &  1.433248e-01 & 7.421972e-05 & 9.401165e-04\\\\\n",
       "\t genderother              & -1.495738e+00 & 7.373868e-04 & 8.005913e-03\\\\\n",
       "\t degree\\_in\\_PosAff         &  7.447843e-02 & 3.853911e-03 & 3.661215e-02\\\\\n",
       "\t degree\\_in\\_Liked          &  5.885013e-02 & 6.579572e-03 & 5.556083e-02\\\\\n",
       "\t gendermale               &  2.036963e-01 & 1.648248e-02 & 1.252669e-01\\\\\n",
       "\t degree\\_in\\_CloseFrds      &  5.065149e-02 & 2.297654e-02 & 1.587470e-01\\\\\n",
       "\t degree\\_out\\_Responsive    &  7.583015e-02 & 3.228096e-02 & 2.044461e-01\\\\\n",
       "\t racewhite                &  3.103139e-01 & 3.903892e-02 & 2.125741e-01\\\\\n",
       "\t degree\\_in\\_INTIMATE       &  4.180768e-02 & 3.915839e-02 & 2.125741e-01\\\\\n",
       "\t degree\\_out\\_NegAff        & -1.305626e-01 & 4.709848e-02 & 2.268036e-01\\\\\n",
       "\t degree\\_in\\_StudyWith      &  5.955247e-02 & 4.774813e-02 & 2.268036e-01\\\\\n",
       "\t dormFaisan               & -6.097550e-01 & 5.461906e-02 & 2.434837e-01\\\\\n",
       "\t empathy\\_Gossip           &  1.714183e-01 & 5.766719e-02 & 2.434837e-01\\\\\n",
       "\t dormSally Ride           &  5.111810e-01 & 6.616253e-02 & 2.646501e-01\\\\\n",
       "\t degree\\_in\\_UNION          &  2.586730e-02 & 8.079672e-02 & 2.919680e-01\\\\\n",
       "\t degree\\_in\\_NegEmoSupp     &  4.641168e-02 & 8.451705e-02 & 2.919680e-01\\\\\n",
       "\t degree\\_in\\_PosEmoSupp     &  5.148551e-02 & 7.804372e-02 & 2.919680e-01\\\\\n",
       "\t dormTwain                &  4.316860e-01 & 9.039257e-02 & 2.986885e-01\\\\\n",
       "\t dormOkada                & -5.350986e-01 & 9.801167e-02 & 3.032138e-01\\\\\n",
       "\t dormOtero                &  4.189891e-01 & 1.037310e-01 & 3.032138e-01\\\\\n",
       "\t degree\\_in\\_Responsive     &  4.759260e-02 & 1.036170e-01 & 3.032138e-01\\\\\n",
       "\t BFI\\_A                    &  6.052139e-02 & 1.136127e-01 & 3.197988e-01\\\\\n",
       "\t BFI\\_O                    &  6.243507e-02 & 1.456096e-01 & 3.688776e-01\\\\\n",
       "\t degree\\_out\\_NegEmoSupp    &  4.835920e-02 & 1.388319e-01 & 3.688776e-01\\\\\n",
       "\t empathy\\_Liked            & -1.443963e-01 & 1.440223e-01 & 3.688776e-01\\\\\n",
       "\t degree\\_out\\_PosAff        &  4.864362e-02 & 1.608423e-01 & 3.820006e-01\\\\\n",
       "\t degree\\_out\\_StudyWith     &  5.031138e-02 & 1.596709e-01 & 3.820006e-01\\\\\n",
       "\t degree\\_in\\_ACQUAINTANCE   &  4.824701e-02 & 1.787319e-01 & 4.116249e-01\\\\\n",
       "\t dormArroyo               &  2.894938e-01 & 2.816514e-01 & 6.295736e-01\\\\\n",
       "\t degree\\_out\\_Liked         &  3.835316e-02 & 3.328839e-01 & 7.228337e-01\\\\\n",
       "\t degree\\_out\\_Gossip        &  3.322877e-02 & 3.468802e-01 & 7.323026e-01\\\\\n",
       "\t degree\\_out\\_ACQUAINTANCE  & -4.039903e-02 & 3.589033e-01 & 7.372067e-01\\\\\n",
       "\t dormNorcliffe            &  2.378226e-01 & 4.013319e-01 & 8.026638e-01\\\\\n",
       "\t degree\\_out\\_EmpSupp       &  3.416598e-02 & 4.145791e-01 & 8.078976e-01\\\\\n",
       "\t racesouth\\_asian          &  1.299660e-01 & 4.785940e-01 & 8.084262e-01\\\\\n",
       "\t degree\\_out\\_INTIMATE      &  1.974817e-02 & 4.875336e-01 & 8.084262e-01\\\\\n",
       "\t degree\\_out\\_CloseFrds     &  2.135656e-02 & 4.893106e-01 & 8.084262e-01\\\\\n",
       "\t degree\\_out\\_PosEmoSupp    &  2.397531e-02 & 4.716731e-01 & 8.084262e-01\\\\\n",
       "\t empathy\\_PosEmoSupp       &  6.562227e-02 & 4.638877e-01 & 8.084262e-01\\\\\n",
       "\t empathy\\_EmpSupp          &  7.151584e-02 & 4.441996e-01 & 8.084262e-01\\\\\n",
       "\t degree\\_in\\_NegAff         & -3.422232e-02 & 4.527825e-01 & 8.084262e-01\\\\\n",
       "\t racehispanic             & -1.140033e-01 & 5.166226e-01 & 8.179858e-01\\\\\n",
       "\t dormRinconada            &  1.763862e-01 & 5.116973e-01 & 8.179858e-01\\\\\n",
       "\t dormLarkin               &  1.366665e-01 & 5.703091e-01 & 8.668699e-01\\\\\n",
       "\t empathy\\_StudyWith        &  5.400922e-02 & 5.702951e-01 & 8.668699e-01\\\\\n",
       "\t degree\\_in\\_Gossip         &  1.711755e-02 & 5.850170e-01 & 8.717900e-01\\\\\n",
       "\t degree\\_in\\_EmpSupp        & -1.524696e-02 & 6.013467e-01 & 8.788914e-01\\\\\n",
       "\t empathy\\_UNION            & -4.744776e-02 & 6.388674e-01 & 8.827986e-01\\\\\n",
       "\t empathy\\_INTIMATE         & -4.585499e-02 & 6.340394e-01 & 8.827986e-01\\\\\n",
       "\t empathy\\_ACQUAINTANCE     & -5.332378e-02 & 6.198738e-01 & 8.827986e-01\\\\\n",
       "\t dormFroSoCo              & -1.123894e-01 & 6.544860e-01 & 8.882310e-01\\\\\n",
       "\t dormRoble                &  9.724519e-02 & 6.908443e-01 & 8.899011e-01\\\\\n",
       "\t intl\\_student             &  5.025582e-02 & 6.833949e-01 & 8.899011e-01\\\\\n",
       "\t empathy\\_CloseFrds        & -3.652359e-02 & 6.869275e-01 & 8.899011e-01\\\\\n",
       "\t raceeast\\_asian           &  4.914058e-02 & 7.562245e-01 & 9.122709e-01\\\\\n",
       "\t dormEucalipto            & -1.111507e-01 & 7.557527e-01 & 9.122709e-01\\\\\n",
       "\t dormJunipero             & -9.040046e-02 & 7.330049e-01 & 9.122709e-01\\\\\n",
       "\t degree\\_out\\_UNION         &  6.154255e-03 & 7.483309e-01 & 9.122709e-01\\\\\n",
       "\t dormZapata               &  1.060936e-01 & 7.820975e-01 & 9.257624e-01\\\\\n",
       "\t empathy\\_NegEmoSupp       &  2.394283e-02 & 7.917705e-01 & 9.257624e-01\\\\\n",
       "\t empathy                  &  1.404412e-02 & 8.225270e-01 & 9.393732e-01\\\\\n",
       "\t empathy\\_NegAff           &  3.061256e-02 & 8.281317e-01 & 9.393732e-01\\\\\n",
       "\t dormLoro                 & -6.367529e-02 & 8.505471e-01 & 9.506115e-01\\\\\n",
       "\t empathy\\_Responsive       &  1.358870e-02 & 8.803754e-01 & 9.696889e-01\\\\\n",
       "\t raceother\\_or\\_mixed       &  1.917614e-02 & 9.014388e-01 & 9.768691e-01\\\\\n",
       "\t dormBurbank              &  1.970447e-02 & 9.383085e-01 & 9.768691e-01\\\\\n",
       "\t dormCedro                &  2.182167e-02 & 9.348168e-01 & 9.768691e-01\\\\\n",
       "\t dormMuwekma-Tah-Ruk      &  3.798578e-02 & 9.281985e-01 & 9.768691e-01\\\\\n",
       "\t dormGavilan              & -2.224323e-02 & 9.557231e-01 & 9.815534e-01\\\\\n",
       "\t empathy\\_PosAff           &  3.132939e-03 & 9.720067e-01 & 9.849668e-01\\\\\n",
       "\t dormUjamaa               & -2.871713e-03 & 9.921860e-01 & 9.921860e-01\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 76 × 4\n",
       "\n",
       "| column &lt;fct&gt; | beta &lt;dbl&gt; | p &lt;dbl&gt; | p_adjusted &lt;dbl&gt; |\n",
       "|---|---|---|---|\n",
       "| wellbeing_composite_t1   |  6.676567e-01 | 2.864762e-73 | 2.177219e-71 |\n",
       "| BFI_N                    | -3.162692e-01 | 1.195296e-25 | 4.542127e-24 |\n",
       "| family_income            |  3.676776e-06 | 5.560889e-10 | 1.141938e-08 |\n",
       "| parent_education_highest |  1.773958e-01 | 6.010200e-10 | 1.141938e-08 |\n",
       "| BFI_E                    |  1.198556e-01 | 2.216161e-05 | 3.368565e-04 |\n",
       "| BFI_C                    |  1.433248e-01 | 7.421972e-05 | 9.401165e-04 |\n",
       "| genderother              | -1.495738e+00 | 7.373868e-04 | 8.005913e-03 |\n",
       "| degree_in_PosAff         |  7.447843e-02 | 3.853911e-03 | 3.661215e-02 |\n",
       "| degree_in_Liked          |  5.885013e-02 | 6.579572e-03 | 5.556083e-02 |\n",
       "| gendermale               |  2.036963e-01 | 1.648248e-02 | 1.252669e-01 |\n",
       "| degree_in_CloseFrds      |  5.065149e-02 | 2.297654e-02 | 1.587470e-01 |\n",
       "| degree_out_Responsive    |  7.583015e-02 | 3.228096e-02 | 2.044461e-01 |\n",
       "| racewhite                |  3.103139e-01 | 3.903892e-02 | 2.125741e-01 |\n",
       "| degree_in_INTIMATE       |  4.180768e-02 | 3.915839e-02 | 2.125741e-01 |\n",
       "| degree_out_NegAff        | -1.305626e-01 | 4.709848e-02 | 2.268036e-01 |\n",
       "| degree_in_StudyWith      |  5.955247e-02 | 4.774813e-02 | 2.268036e-01 |\n",
       "| dormFaisan               | -6.097550e-01 | 5.461906e-02 | 2.434837e-01 |\n",
       "| empathy_Gossip           |  1.714183e-01 | 5.766719e-02 | 2.434837e-01 |\n",
       "| dormSally Ride           |  5.111810e-01 | 6.616253e-02 | 2.646501e-01 |\n",
       "| degree_in_UNION          |  2.586730e-02 | 8.079672e-02 | 2.919680e-01 |\n",
       "| degree_in_NegEmoSupp     |  4.641168e-02 | 8.451705e-02 | 2.919680e-01 |\n",
       "| degree_in_PosEmoSupp     |  5.148551e-02 | 7.804372e-02 | 2.919680e-01 |\n",
       "| dormTwain                |  4.316860e-01 | 9.039257e-02 | 2.986885e-01 |\n",
       "| dormOkada                | -5.350986e-01 | 9.801167e-02 | 3.032138e-01 |\n",
       "| dormOtero                |  4.189891e-01 | 1.037310e-01 | 3.032138e-01 |\n",
       "| degree_in_Responsive     |  4.759260e-02 | 1.036170e-01 | 3.032138e-01 |\n",
       "| BFI_A                    |  6.052139e-02 | 1.136127e-01 | 3.197988e-01 |\n",
       "| BFI_O                    |  6.243507e-02 | 1.456096e-01 | 3.688776e-01 |\n",
       "| degree_out_NegEmoSupp    |  4.835920e-02 | 1.388319e-01 | 3.688776e-01 |\n",
       "| empathy_Liked            | -1.443963e-01 | 1.440223e-01 | 3.688776e-01 |\n",
       "| degree_out_PosAff        |  4.864362e-02 | 1.608423e-01 | 3.820006e-01 |\n",
       "| degree_out_StudyWith     |  5.031138e-02 | 1.596709e-01 | 3.820006e-01 |\n",
       "| degree_in_ACQUAINTANCE   |  4.824701e-02 | 1.787319e-01 | 4.116249e-01 |\n",
       "| dormArroyo               |  2.894938e-01 | 2.816514e-01 | 6.295736e-01 |\n",
       "| degree_out_Liked         |  3.835316e-02 | 3.328839e-01 | 7.228337e-01 |\n",
       "| degree_out_Gossip        |  3.322877e-02 | 3.468802e-01 | 7.323026e-01 |\n",
       "| degree_out_ACQUAINTANCE  | -4.039903e-02 | 3.589033e-01 | 7.372067e-01 |\n",
       "| dormNorcliffe            |  2.378226e-01 | 4.013319e-01 | 8.026638e-01 |\n",
       "| degree_out_EmpSupp       |  3.416598e-02 | 4.145791e-01 | 8.078976e-01 |\n",
       "| racesouth_asian          |  1.299660e-01 | 4.785940e-01 | 8.084262e-01 |\n",
       "| degree_out_INTIMATE      |  1.974817e-02 | 4.875336e-01 | 8.084262e-01 |\n",
       "| degree_out_CloseFrds     |  2.135656e-02 | 4.893106e-01 | 8.084262e-01 |\n",
       "| degree_out_PosEmoSupp    |  2.397531e-02 | 4.716731e-01 | 8.084262e-01 |\n",
       "| empathy_PosEmoSupp       |  6.562227e-02 | 4.638877e-01 | 8.084262e-01 |\n",
       "| empathy_EmpSupp          |  7.151584e-02 | 4.441996e-01 | 8.084262e-01 |\n",
       "| degree_in_NegAff         | -3.422232e-02 | 4.527825e-01 | 8.084262e-01 |\n",
       "| racehispanic             | -1.140033e-01 | 5.166226e-01 | 8.179858e-01 |\n",
       "| dormRinconada            |  1.763862e-01 | 5.116973e-01 | 8.179858e-01 |\n",
       "| dormLarkin               |  1.366665e-01 | 5.703091e-01 | 8.668699e-01 |\n",
       "| empathy_StudyWith        |  5.400922e-02 | 5.702951e-01 | 8.668699e-01 |\n",
       "| degree_in_Gossip         |  1.711755e-02 | 5.850170e-01 | 8.717900e-01 |\n",
       "| degree_in_EmpSupp        | -1.524696e-02 | 6.013467e-01 | 8.788914e-01 |\n",
       "| empathy_UNION            | -4.744776e-02 | 6.388674e-01 | 8.827986e-01 |\n",
       "| empathy_INTIMATE         | -4.585499e-02 | 6.340394e-01 | 8.827986e-01 |\n",
       "| empathy_ACQUAINTANCE     | -5.332378e-02 | 6.198738e-01 | 8.827986e-01 |\n",
       "| dormFroSoCo              | -1.123894e-01 | 6.544860e-01 | 8.882310e-01 |\n",
       "| dormRoble                |  9.724519e-02 | 6.908443e-01 | 8.899011e-01 |\n",
       "| intl_student             |  5.025582e-02 | 6.833949e-01 | 8.899011e-01 |\n",
       "| empathy_CloseFrds        | -3.652359e-02 | 6.869275e-01 | 8.899011e-01 |\n",
       "| raceeast_asian           |  4.914058e-02 | 7.562245e-01 | 9.122709e-01 |\n",
       "| dormEucalipto            | -1.111507e-01 | 7.557527e-01 | 9.122709e-01 |\n",
       "| dormJunipero             | -9.040046e-02 | 7.330049e-01 | 9.122709e-01 |\n",
       "| degree_out_UNION         |  6.154255e-03 | 7.483309e-01 | 9.122709e-01 |\n",
       "| dormZapata               |  1.060936e-01 | 7.820975e-01 | 9.257624e-01 |\n",
       "| empathy_NegEmoSupp       |  2.394283e-02 | 7.917705e-01 | 9.257624e-01 |\n",
       "| empathy                  |  1.404412e-02 | 8.225270e-01 | 9.393732e-01 |\n",
       "| empathy_NegAff           |  3.061256e-02 | 8.281317e-01 | 9.393732e-01 |\n",
       "| dormLoro                 | -6.367529e-02 | 8.505471e-01 | 9.506115e-01 |\n",
       "| empathy_Responsive       |  1.358870e-02 | 8.803754e-01 | 9.696889e-01 |\n",
       "| raceother_or_mixed       |  1.917614e-02 | 9.014388e-01 | 9.768691e-01 |\n",
       "| dormBurbank              |  1.970447e-02 | 9.383085e-01 | 9.768691e-01 |\n",
       "| dormCedro                |  2.182167e-02 | 9.348168e-01 | 9.768691e-01 |\n",
       "| dormMuwekma-Tah-Ruk      |  3.798578e-02 | 9.281985e-01 | 9.768691e-01 |\n",
       "| dormGavilan              | -2.224323e-02 | 9.557231e-01 | 9.815534e-01 |\n",
       "| empathy_PosAff           |  3.132939e-03 | 9.720067e-01 | 9.849668e-01 |\n",
       "| dormUjamaa               | -2.871713e-03 | 9.921860e-01 | 9.921860e-01 |\n",
       "\n"
      ],
      "text/plain": [
       "   column                   beta          p            p_adjusted  \n",
       "1  wellbeing_composite_t1    6.676567e-01 2.864762e-73 2.177219e-71\n",
       "2  BFI_N                    -3.162692e-01 1.195296e-25 4.542127e-24\n",
       "3  family_income             3.676776e-06 5.560889e-10 1.141938e-08\n",
       "4  parent_education_highest  1.773958e-01 6.010200e-10 1.141938e-08\n",
       "5  BFI_E                     1.198556e-01 2.216161e-05 3.368565e-04\n",
       "6  BFI_C                     1.433248e-01 7.421972e-05 9.401165e-04\n",
       "7  genderother              -1.495738e+00 7.373868e-04 8.005913e-03\n",
       "8  degree_in_PosAff          7.447843e-02 3.853911e-03 3.661215e-02\n",
       "9  degree_in_Liked           5.885013e-02 6.579572e-03 5.556083e-02\n",
       "10 gendermale                2.036963e-01 1.648248e-02 1.252669e-01\n",
       "11 degree_in_CloseFrds       5.065149e-02 2.297654e-02 1.587470e-01\n",
       "12 degree_out_Responsive     7.583015e-02 3.228096e-02 2.044461e-01\n",
       "13 racewhite                 3.103139e-01 3.903892e-02 2.125741e-01\n",
       "14 degree_in_INTIMATE        4.180768e-02 3.915839e-02 2.125741e-01\n",
       "15 degree_out_NegAff        -1.305626e-01 4.709848e-02 2.268036e-01\n",
       "16 degree_in_StudyWith       5.955247e-02 4.774813e-02 2.268036e-01\n",
       "17 dormFaisan               -6.097550e-01 5.461906e-02 2.434837e-01\n",
       "18 empathy_Gossip            1.714183e-01 5.766719e-02 2.434837e-01\n",
       "19 dormSally Ride            5.111810e-01 6.616253e-02 2.646501e-01\n",
       "20 degree_in_UNION           2.586730e-02 8.079672e-02 2.919680e-01\n",
       "21 degree_in_NegEmoSupp      4.641168e-02 8.451705e-02 2.919680e-01\n",
       "22 degree_in_PosEmoSupp      5.148551e-02 7.804372e-02 2.919680e-01\n",
       "23 dormTwain                 4.316860e-01 9.039257e-02 2.986885e-01\n",
       "24 dormOkada                -5.350986e-01 9.801167e-02 3.032138e-01\n",
       "25 dormOtero                 4.189891e-01 1.037310e-01 3.032138e-01\n",
       "26 degree_in_Responsive      4.759260e-02 1.036170e-01 3.032138e-01\n",
       "27 BFI_A                     6.052139e-02 1.136127e-01 3.197988e-01\n",
       "28 BFI_O                     6.243507e-02 1.456096e-01 3.688776e-01\n",
       "29 degree_out_NegEmoSupp     4.835920e-02 1.388319e-01 3.688776e-01\n",
       "30 empathy_Liked            -1.443963e-01 1.440223e-01 3.688776e-01\n",
       "31 degree_out_PosAff         4.864362e-02 1.608423e-01 3.820006e-01\n",
       "32 degree_out_StudyWith      5.031138e-02 1.596709e-01 3.820006e-01\n",
       "33 degree_in_ACQUAINTANCE    4.824701e-02 1.787319e-01 4.116249e-01\n",
       "34 dormArroyo                2.894938e-01 2.816514e-01 6.295736e-01\n",
       "35 degree_out_Liked          3.835316e-02 3.328839e-01 7.228337e-01\n",
       "36 degree_out_Gossip         3.322877e-02 3.468802e-01 7.323026e-01\n",
       "37 degree_out_ACQUAINTANCE  -4.039903e-02 3.589033e-01 7.372067e-01\n",
       "38 dormNorcliffe             2.378226e-01 4.013319e-01 8.026638e-01\n",
       "39 degree_out_EmpSupp        3.416598e-02 4.145791e-01 8.078976e-01\n",
       "40 racesouth_asian           1.299660e-01 4.785940e-01 8.084262e-01\n",
       "41 degree_out_INTIMATE       1.974817e-02 4.875336e-01 8.084262e-01\n",
       "42 degree_out_CloseFrds      2.135656e-02 4.893106e-01 8.084262e-01\n",
       "43 degree_out_PosEmoSupp     2.397531e-02 4.716731e-01 8.084262e-01\n",
       "44 empathy_PosEmoSupp        6.562227e-02 4.638877e-01 8.084262e-01\n",
       "45 empathy_EmpSupp           7.151584e-02 4.441996e-01 8.084262e-01\n",
       "46 degree_in_NegAff         -3.422232e-02 4.527825e-01 8.084262e-01\n",
       "47 racehispanic             -1.140033e-01 5.166226e-01 8.179858e-01\n",
       "48 dormRinconada             1.763862e-01 5.116973e-01 8.179858e-01\n",
       "49 dormLarkin                1.366665e-01 5.703091e-01 8.668699e-01\n",
       "50 empathy_StudyWith         5.400922e-02 5.702951e-01 8.668699e-01\n",
       "51 degree_in_Gossip          1.711755e-02 5.850170e-01 8.717900e-01\n",
       "52 degree_in_EmpSupp        -1.524696e-02 6.013467e-01 8.788914e-01\n",
       "53 empathy_UNION            -4.744776e-02 6.388674e-01 8.827986e-01\n",
       "54 empathy_INTIMATE         -4.585499e-02 6.340394e-01 8.827986e-01\n",
       "55 empathy_ACQUAINTANCE     -5.332378e-02 6.198738e-01 8.827986e-01\n",
       "56 dormFroSoCo              -1.123894e-01 6.544860e-01 8.882310e-01\n",
       "57 dormRoble                 9.724519e-02 6.908443e-01 8.899011e-01\n",
       "58 intl_student              5.025582e-02 6.833949e-01 8.899011e-01\n",
       "59 empathy_CloseFrds        -3.652359e-02 6.869275e-01 8.899011e-01\n",
       "60 raceeast_asian            4.914058e-02 7.562245e-01 9.122709e-01\n",
       "61 dormEucalipto            -1.111507e-01 7.557527e-01 9.122709e-01\n",
       "62 dormJunipero             -9.040046e-02 7.330049e-01 9.122709e-01\n",
       "63 degree_out_UNION          6.154255e-03 7.483309e-01 9.122709e-01\n",
       "64 dormZapata                1.060936e-01 7.820975e-01 9.257624e-01\n",
       "65 empathy_NegEmoSupp        2.394283e-02 7.917705e-01 9.257624e-01\n",
       "66 empathy                   1.404412e-02 8.225270e-01 9.393732e-01\n",
       "67 empathy_NegAff            3.061256e-02 8.281317e-01 9.393732e-01\n",
       "68 dormLoro                 -6.367529e-02 8.505471e-01 9.506115e-01\n",
       "69 empathy_Responsive        1.358870e-02 8.803754e-01 9.696889e-01\n",
       "70 raceother_or_mixed        1.917614e-02 9.014388e-01 9.768691e-01\n",
       "71 dormBurbank               1.970447e-02 9.383085e-01 9.768691e-01\n",
       "72 dormCedro                 2.182167e-02 9.348168e-01 9.768691e-01\n",
       "73 dormMuwekma-Tah-Ruk       3.798578e-02 9.281985e-01 9.768691e-01\n",
       "74 dormGavilan              -2.224323e-02 9.557231e-01 9.815534e-01\n",
       "75 empathy_PosAff            3.132939e-03 9.720067e-01 9.849668e-01\n",
       "76 dormUjamaa               -2.871713e-03 9.921860e-01 9.921860e-01"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "frame = NULL\n",
    "for (col in names(df)) {\n",
    "    # The only items that end with 't1' and 't2' are well-being\n",
    "    # measures, which we don't want to include *except* for the\n",
    "    # t1 composite well being. That is, we don't want to include\n",
    "    # life_satisfaction_t1 or life_satisfaction_t2 since they are\n",
    "    # parts of the wellbeing_composite measure of interest.\n",
    "    if (endsWith(col, 't2')) next;\n",
    "    if (endsWith(col, 't1') & col != 'wellbeing_composite_t1') next;\n",
    "    model = lm(as.formula(paste('wellbeing_composite_t2 ~', col)), df)\n",
    "    params = summary(model)$coefficients\n",
    "    for (i in 2:length(row.names(params))) {\n",
    "        b = params[i, 1]\n",
    "        p = params[i, 4]\n",
    "        frame = rbind(frame, c(row.names(params)[i], b, p))\n",
    "    }\n",
    "}\n",
    "frame = as.data.frame(frame)\n",
    "names(frame) = c('column', 'beta', 'p')\n",
    "frame$beta = as.numeric(as.character(frame$beta))\n",
    "frame$p = as.numeric(as.character(frame$p))\n",
    "frame$p_adjusted = p.adjust(frame$p, method=\"BH\")\n",
    "frame %>% arrange(p_adjusted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "r-3.6.2"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
