{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network statistics do not predict t2 wellbeing outright, and thus cannot mediate anything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Configuration\" data-toc-modified-id=\"Configuration-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Configuration</a></span></li><li><span><a href=\"#Import-and-load\" data-toc-modified-id=\"Import-and-load-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Import and load</a></span></li><li><span><a href=\"#Run-the-base-model-predicting-t2-from-t1-wellbeing\" data-toc-modified-id=\"Run-the-base-model-predicting-t2-from-t1-wellbeing-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Run the base model predicting t2 from t1 wellbeing</a></span></li><li><span><a href=\"#Can-any-network-variable-predict-t2-wellbeing-outright?\" data-toc-modified-id=\"Can-any-network-variable-predict-t2-wellbeing-outright?-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Can any network variable predict t2 wellbeing outright?</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = '../data/2019–2020/postprocessed/df_Rcleaned_train.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(car)\n",
    "library(tidyverse)\n",
    "library(hexbin)\n",
    "library(mice)\n",
    "library(nlme)\n",
    "library(lme4)\n",
    "library(lmerTest)\n",
    "\n",
    "# Display more data in the Jupyter notebook\n",
    "options(repr.matrix.max.cols=500, repr.matrix.max.rows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>560</li>\n",
       "\t<li>61</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 560\n",
       "\\item 61\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 560\n",
       "2. 61\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 560  61"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.frame: 6 × 61</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>X</th><th scope=col>PID</th><th scope=col>gender</th><th scope=col>race</th><th scope=col>dorm</th><th scope=col>life_satisfaction_t1</th><th scope=col>empathy</th><th scope=col>loneliness_t1</th><th scope=col>stress_t1</th><th scope=col>BFI_E</th><th scope=col>BFI_A</th><th scope=col>BFI_C</th><th scope=col>BFI_N</th><th scope=col>BFI_O</th><th scope=col>intl_student</th><th scope=col>family_income</th><th scope=col>life_satisfaction_t2</th><th scope=col>loneliness_t2</th><th scope=col>stress_t2</th><th scope=col>parent_education_highest</th><th scope=col>wellbeing_composite_t1</th><th scope=col>wellbeing_composite_t2</th><th scope=col>degree_in_UNION</th><th scope=col>degree_out_UNION</th><th scope=col>empathy_UNION</th><th scope=col>degree_in_INTIMATE</th><th scope=col>degree_out_INTIMATE</th><th scope=col>empathy_INTIMATE</th><th scope=col>degree_in_ACQUAINTANCE</th><th scope=col>degree_out_ACQUAINTANCE</th><th scope=col>empathy_ACQUAINTANCE</th><th scope=col>degree_in_CloseFrds</th><th scope=col>degree_out_CloseFrds</th><th scope=col>empathy_CloseFrds</th><th scope=col>degree_in_NegEmoSupp</th><th scope=col>degree_out_NegEmoSupp</th><th scope=col>empathy_NegEmoSupp</th><th scope=col>degree_in_PosEmoSupp</th><th scope=col>degree_out_PosEmoSupp</th><th scope=col>empathy_PosEmoSupp</th><th scope=col>degree_in_Responsive</th><th scope=col>degree_out_Responsive</th><th scope=col>empathy_Responsive</th><th scope=col>degree_in_EmpSupp</th><th scope=col>degree_out_EmpSupp</th><th scope=col>empathy_EmpSupp</th><th scope=col>degree_in_PosAff</th><th scope=col>degree_out_PosAff</th><th scope=col>empathy_PosAff</th><th scope=col>degree_in_NegAff</th><th scope=col>degree_out_NegAff</th><th scope=col>empathy_NegAff</th><th scope=col>degree_in_Gossip</th><th scope=col>degree_out_Gossip</th><th scope=col>empathy_Gossip</th><th scope=col>degree_in_Liked</th><th scope=col>degree_out_Liked</th><th scope=col>empathy_Liked</th><th scope=col>degree_in_StudyWith</th><th scope=col>degree_out_StudyWith</th><th scope=col>empathy_StudyWith</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>1</td><td>1001</td><td>male  </td><td>south_asian   </td><td>Rinconada</td><td>4.833333</td><td>6.375</td><td>1.333333</td><td>2.5</td><td>6.5</td><td>6.0</td><td>5.5</td><td>3.5</td><td>6.5</td><td>0</td><td> 70000</td><td>6.166667</td><td>2.000000</td><td>1.5</td><td>5</td><td>-0.05862651</td><td> 0.83993983</td><td>3</td><td>0</td><td>      NA</td><td>1</td><td>0</td><td>     NA</td><td>1</td><td>0</td><td>   NA</td><td>1</td><td>0</td><td>     NA</td><td>0</td><td>0</td><td>      NA</td><td>2</td><td>0</td><td>    NA</td><td>0</td><td>0</td><td>    NA</td><td>0</td><td>0</td><td>    NA</td><td>1</td><td>0</td><td>     NA</td><td>0</td><td>0</td><td> NA</td><td>1</td><td>0</td><td>      NA</td><td>1</td><td>0</td><td>   NA</td><td>1</td><td>0</td><td>    NA</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>2</td><td>1047</td><td>female</td><td>other_or_mixed</td><td>Twain    </td><td>5.333333</td><td>6.500</td><td>2.000000</td><td>2.5</td><td>5.5</td><td>6.5</td><td>7.0</td><td>3.0</td><td>6.5</td><td>0</td><td>210000</td><td>5.166667</td><td>2.000000</td><td>3.0</td><td>5</td><td>-0.44690440</td><td>-0.66253028</td><td>6</td><td>4</td><td>6.437500</td><td>4</td><td>4</td><td>6.43750</td><td>0</td><td>1</td><td>6.125</td><td>4</td><td>4</td><td>6.43750</td><td>4</td><td>3</td><td>6.541667</td><td>3</td><td>4</td><td>6.4375</td><td>2</td><td>2</td><td>6.5625</td><td>1</td><td>2</td><td>6.5625</td><td>3</td><td>4</td><td>6.43750</td><td>1</td><td>1</td><td>6.5</td><td>4</td><td>3</td><td>6.541667</td><td>1</td><td>2</td><td>6.000</td><td>2</td><td>1</td><td>6.6250</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>3</td><td>1078</td><td>female</td><td>east_asian    </td><td>Loro     </td><td>5.500000</td><td>6.250</td><td>1.333333</td><td>2.0</td><td>4.5</td><td>6.5</td><td>6.5</td><td>1.5</td><td>6.5</td><td>0</td><td>130000</td><td>5.500000</td><td>2.333333</td><td>3.0</td><td>6</td><td> 0.61446613</td><td>-0.79801599</td><td>1</td><td>0</td><td>      NA</td><td>1</td><td>0</td><td>     NA</td><td>1</td><td>0</td><td>   NA</td><td>1</td><td>0</td><td>     NA</td><td>0</td><td>0</td><td>      NA</td><td>1</td><td>0</td><td>    NA</td><td>1</td><td>0</td><td>    NA</td><td>0</td><td>0</td><td>    NA</td><td>0</td><td>0</td><td>     NA</td><td>0</td><td>1</td><td>5.0</td><td>0</td><td>0</td><td>      NA</td><td>0</td><td>0</td><td>   NA</td><td>0</td><td>0</td><td>    NA</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>4</td><td>1097</td><td>male  </td><td>east_asian    </td><td>Otero    </td><td>6.000000</td><td>5.000</td><td>2.000000</td><td>2.0</td><td>3.0</td><td>4.0</td><td>5.5</td><td>1.5</td><td>4.5</td><td>0</td><td> 90000</td><td>5.833333</td><td>2.000000</td><td>2.5</td><td>6</td><td> 0.22618824</td><td>-0.04126834</td><td>6</td><td>4</td><td>5.812500</td><td>3</td><td>1</td><td>5.62500</td><td>2</td><td>0</td><td>   NA</td><td>2</td><td>1</td><td>5.62500</td><td>1</td><td>1</td><td>5.625000</td><td>1</td><td>1</td><td>6.6250</td><td>0</td><td>1</td><td>5.6250</td><td>0</td><td>1</td><td>4.8750</td><td>1</td><td>1</td><td>6.12500</td><td>0</td><td>0</td><td> NA</td><td>0</td><td>1</td><td>5.625000</td><td>0</td><td>1</td><td>6.000</td><td>3</td><td>1</td><td>6.6250</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>5</td><td>1105</td><td>female</td><td>white         </td><td>Larkin   </td><td>6.666667</td><td>5.750</td><td>2.000000</td><td>2.0</td><td>4.5</td><td>5.0</td><td>5.0</td><td>2.0</td><td>4.5</td><td>0</td><td>170000</td><td>6.666667</td><td>2.000000</td><td>2.0</td><td>4</td><td> 0.48041399</td><td> 0.64021289</td><td>6</td><td>9</td><td>5.847222</td><td>4</td><td>5</td><td>6.02500</td><td>2</td><td>2</td><td>5.875</td><td>3</td><td>4</td><td>5.81250</td><td>2</td><td>3</td><td>6.125000</td><td>2</td><td>2</td><td>5.3750</td><td>2</td><td>2</td><td>6.3125</td><td>1</td><td>4</td><td>6.1875</td><td>2</td><td>1</td><td>6.87500</td><td>1</td><td>0</td><td> NA</td><td>2</td><td>3</td><td>5.750000</td><td>1</td><td>1</td><td>5.125</td><td>4</td><td>2</td><td>6.1875</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>6</td><td>1110</td><td>female</td><td>white         </td><td>Larkin   </td><td>5.166667</td><td>6.750</td><td>2.000000</td><td>2.0</td><td>6.5</td><td>5.0</td><td>6.0</td><td>2.0</td><td>4.5</td><td>0</td><td>110000</td><td>6.833333</td><td>1.666667</td><td>2.0</td><td>5</td><td>-0.09159395</td><td> 0.95635646</td><td>4</td><td>5</td><td>5.550000</td><td>4</td><td>4</td><td>5.40625</td><td>1</td><td>0</td><td>   NA</td><td>3</td><td>4</td><td>5.40625</td><td>3</td><td>4</td><td>5.406250</td><td>3</td><td>2</td><td>6.0625</td><td>2</td><td>2</td><td>5.7500</td><td>3</td><td>1</td><td>6.0000</td><td>2</td><td>4</td><td>5.78125</td><td>0</td><td>0</td><td> NA</td><td>3</td><td>4</td><td>5.406250</td><td>4</td><td>1</td><td>6.000</td><td>3</td><td>4</td><td>5.4375</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 61\n",
       "\\begin{tabular}{r|lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll}\n",
       "  & X & PID & gender & race & dorm & life\\_satisfaction\\_t1 & empathy & loneliness\\_t1 & stress\\_t1 & BFI\\_E & BFI\\_A & BFI\\_C & BFI\\_N & BFI\\_O & intl\\_student & family\\_income & life\\_satisfaction\\_t2 & loneliness\\_t2 & stress\\_t2 & parent\\_education\\_highest & wellbeing\\_composite\\_t1 & wellbeing\\_composite\\_t2 & degree\\_in\\_UNION & degree\\_out\\_UNION & empathy\\_UNION & degree\\_in\\_INTIMATE & degree\\_out\\_INTIMATE & empathy\\_INTIMATE & degree\\_in\\_ACQUAINTANCE & degree\\_out\\_ACQUAINTANCE & empathy\\_ACQUAINTANCE & degree\\_in\\_CloseFrds & degree\\_out\\_CloseFrds & empathy\\_CloseFrds & degree\\_in\\_NegEmoSupp & degree\\_out\\_NegEmoSupp & empathy\\_NegEmoSupp & degree\\_in\\_PosEmoSupp & degree\\_out\\_PosEmoSupp & empathy\\_PosEmoSupp & degree\\_in\\_Responsive & degree\\_out\\_Responsive & empathy\\_Responsive & degree\\_in\\_EmpSupp & degree\\_out\\_EmpSupp & empathy\\_EmpSupp & degree\\_in\\_PosAff & degree\\_out\\_PosAff & empathy\\_PosAff & degree\\_in\\_NegAff & degree\\_out\\_NegAff & empathy\\_NegAff & degree\\_in\\_Gossip & degree\\_out\\_Gossip & empathy\\_Gossip & degree\\_in\\_Liked & degree\\_out\\_Liked & empathy\\_Liked & degree\\_in\\_StudyWith & degree\\_out\\_StudyWith & empathy\\_StudyWith\\\\\n",
       "  & <int> & <int> & <fct> & <fct> & <fct> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <int> & <int> & <dbl> & <dbl> & <dbl> & <int> & <dbl> & <dbl> & <int> & <int> & <dbl> & <int> & <int> & <dbl> & <int> & <int> & <dbl> & <int> & <int> & <dbl> & <int> & <int> & <dbl> & <int> & <int> & <dbl> & <int> & <int> & <dbl> & <int> & <int> & <dbl> & <int> & <int> & <dbl> & <int> & <int> & <dbl> & <int> & <int> & <dbl> & <int> & <int> & <dbl> & <int> & <int> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & 1 & 1001 & male   & south\\_asian    & Rinconada & 4.833333 & 6.375 & 1.333333 & 2.5 & 6.5 & 6.0 & 5.5 & 3.5 & 6.5 & 0 &  70000 & 6.166667 & 2.000000 & 1.5 & 5 & -0.05862651 &  0.83993983 & 3 & 0 &       NA & 1 & 0 &      NA & 1 & 0 &    NA & 1 & 0 &      NA & 0 & 0 &       NA & 2 & 0 &     NA & 0 & 0 &     NA & 0 & 0 &     NA & 1 & 0 &      NA & 0 & 0 &  NA & 1 & 0 &       NA & 1 & 0 &    NA & 1 & 0 &     NA\\\\\n",
       "\t2 & 2 & 1047 & female & other\\_or\\_mixed & Twain     & 5.333333 & 6.500 & 2.000000 & 2.5 & 5.5 & 6.5 & 7.0 & 3.0 & 6.5 & 0 & 210000 & 5.166667 & 2.000000 & 3.0 & 5 & -0.44690440 & -0.66253028 & 6 & 4 & 6.437500 & 4 & 4 & 6.43750 & 0 & 1 & 6.125 & 4 & 4 & 6.43750 & 4 & 3 & 6.541667 & 3 & 4 & 6.4375 & 2 & 2 & 6.5625 & 1 & 2 & 6.5625 & 3 & 4 & 6.43750 & 1 & 1 & 6.5 & 4 & 3 & 6.541667 & 1 & 2 & 6.000 & 2 & 1 & 6.6250\\\\\n",
       "\t3 & 3 & 1078 & female & east\\_asian     & Loro      & 5.500000 & 6.250 & 1.333333 & 2.0 & 4.5 & 6.5 & 6.5 & 1.5 & 6.5 & 0 & 130000 & 5.500000 & 2.333333 & 3.0 & 6 &  0.61446613 & -0.79801599 & 1 & 0 &       NA & 1 & 0 &      NA & 1 & 0 &    NA & 1 & 0 &      NA & 0 & 0 &       NA & 1 & 0 &     NA & 1 & 0 &     NA & 0 & 0 &     NA & 0 & 0 &      NA & 0 & 1 & 5.0 & 0 & 0 &       NA & 0 & 0 &    NA & 0 & 0 &     NA\\\\\n",
       "\t4 & 4 & 1097 & male   & east\\_asian     & Otero     & 6.000000 & 5.000 & 2.000000 & 2.0 & 3.0 & 4.0 & 5.5 & 1.5 & 4.5 & 0 &  90000 & 5.833333 & 2.000000 & 2.5 & 6 &  0.22618824 & -0.04126834 & 6 & 4 & 5.812500 & 3 & 1 & 5.62500 & 2 & 0 &    NA & 2 & 1 & 5.62500 & 1 & 1 & 5.625000 & 1 & 1 & 6.6250 & 0 & 1 & 5.6250 & 0 & 1 & 4.8750 & 1 & 1 & 6.12500 & 0 & 0 &  NA & 0 & 1 & 5.625000 & 0 & 1 & 6.000 & 3 & 1 & 6.6250\\\\\n",
       "\t5 & 5 & 1105 & female & white          & Larkin    & 6.666667 & 5.750 & 2.000000 & 2.0 & 4.5 & 5.0 & 5.0 & 2.0 & 4.5 & 0 & 170000 & 6.666667 & 2.000000 & 2.0 & 4 &  0.48041399 &  0.64021289 & 6 & 9 & 5.847222 & 4 & 5 & 6.02500 & 2 & 2 & 5.875 & 3 & 4 & 5.81250 & 2 & 3 & 6.125000 & 2 & 2 & 5.3750 & 2 & 2 & 6.3125 & 1 & 4 & 6.1875 & 2 & 1 & 6.87500 & 1 & 0 &  NA & 2 & 3 & 5.750000 & 1 & 1 & 5.125 & 4 & 2 & 6.1875\\\\\n",
       "\t6 & 6 & 1110 & female & white          & Larkin    & 5.166667 & 6.750 & 2.000000 & 2.0 & 6.5 & 5.0 & 6.0 & 2.0 & 4.5 & 0 & 110000 & 6.833333 & 1.666667 & 2.0 & 5 & -0.09159395 &  0.95635646 & 4 & 5 & 5.550000 & 4 & 4 & 5.40625 & 1 & 0 &    NA & 3 & 4 & 5.40625 & 3 & 4 & 5.406250 & 3 & 2 & 6.0625 & 2 & 2 & 5.7500 & 3 & 1 & 6.0000 & 2 & 4 & 5.78125 & 0 & 0 &  NA & 3 & 4 & 5.406250 & 4 & 1 & 6.000 & 3 & 4 & 5.4375\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 61\n",
       "\n",
       "| <!--/--> | X &lt;int&gt; | PID &lt;int&gt; | gender &lt;fct&gt; | race &lt;fct&gt; | dorm &lt;fct&gt; | life_satisfaction_t1 &lt;dbl&gt; | empathy &lt;dbl&gt; | loneliness_t1 &lt;dbl&gt; | stress_t1 &lt;dbl&gt; | BFI_E &lt;dbl&gt; | BFI_A &lt;dbl&gt; | BFI_C &lt;dbl&gt; | BFI_N &lt;dbl&gt; | BFI_O &lt;dbl&gt; | intl_student &lt;int&gt; | family_income &lt;int&gt; | life_satisfaction_t2 &lt;dbl&gt; | loneliness_t2 &lt;dbl&gt; | stress_t2 &lt;dbl&gt; | parent_education_highest &lt;int&gt; | wellbeing_composite_t1 &lt;dbl&gt; | wellbeing_composite_t2 &lt;dbl&gt; | degree_in_UNION &lt;int&gt; | degree_out_UNION &lt;int&gt; | empathy_UNION &lt;dbl&gt; | degree_in_INTIMATE &lt;int&gt; | degree_out_INTIMATE &lt;int&gt; | empathy_INTIMATE &lt;dbl&gt; | degree_in_ACQUAINTANCE &lt;int&gt; | degree_out_ACQUAINTANCE &lt;int&gt; | empathy_ACQUAINTANCE &lt;dbl&gt; | degree_in_CloseFrds &lt;int&gt; | degree_out_CloseFrds &lt;int&gt; | empathy_CloseFrds &lt;dbl&gt; | degree_in_NegEmoSupp &lt;int&gt; | degree_out_NegEmoSupp &lt;int&gt; | empathy_NegEmoSupp &lt;dbl&gt; | degree_in_PosEmoSupp &lt;int&gt; | degree_out_PosEmoSupp &lt;int&gt; | empathy_PosEmoSupp &lt;dbl&gt; | degree_in_Responsive &lt;int&gt; | degree_out_Responsive &lt;int&gt; | empathy_Responsive &lt;dbl&gt; | degree_in_EmpSupp &lt;int&gt; | degree_out_EmpSupp &lt;int&gt; | empathy_EmpSupp &lt;dbl&gt; | degree_in_PosAff &lt;int&gt; | degree_out_PosAff &lt;int&gt; | empathy_PosAff &lt;dbl&gt; | degree_in_NegAff &lt;int&gt; | degree_out_NegAff &lt;int&gt; | empathy_NegAff &lt;dbl&gt; | degree_in_Gossip &lt;int&gt; | degree_out_Gossip &lt;int&gt; | empathy_Gossip &lt;dbl&gt; | degree_in_Liked &lt;int&gt; | degree_out_Liked &lt;int&gt; | empathy_Liked &lt;dbl&gt; | degree_in_StudyWith &lt;int&gt; | degree_out_StudyWith &lt;int&gt; | empathy_StudyWith &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1 | 1 | 1001 | male   | south_asian    | Rinconada | 4.833333 | 6.375 | 1.333333 | 2.5 | 6.5 | 6.0 | 5.5 | 3.5 | 6.5 | 0 |  70000 | 6.166667 | 2.000000 | 1.5 | 5 | -0.05862651 |  0.83993983 | 3 | 0 |       NA | 1 | 0 |      NA | 1 | 0 |    NA | 1 | 0 |      NA | 0 | 0 |       NA | 2 | 0 |     NA | 0 | 0 |     NA | 0 | 0 |     NA | 1 | 0 |      NA | 0 | 0 |  NA | 1 | 0 |       NA | 1 | 0 |    NA | 1 | 0 |     NA |\n",
       "| 2 | 2 | 1047 | female | other_or_mixed | Twain     | 5.333333 | 6.500 | 2.000000 | 2.5 | 5.5 | 6.5 | 7.0 | 3.0 | 6.5 | 0 | 210000 | 5.166667 | 2.000000 | 3.0 | 5 | -0.44690440 | -0.66253028 | 6 | 4 | 6.437500 | 4 | 4 | 6.43750 | 0 | 1 | 6.125 | 4 | 4 | 6.43750 | 4 | 3 | 6.541667 | 3 | 4 | 6.4375 | 2 | 2 | 6.5625 | 1 | 2 | 6.5625 | 3 | 4 | 6.43750 | 1 | 1 | 6.5 | 4 | 3 | 6.541667 | 1 | 2 | 6.000 | 2 | 1 | 6.6250 |\n",
       "| 3 | 3 | 1078 | female | east_asian     | Loro      | 5.500000 | 6.250 | 1.333333 | 2.0 | 4.5 | 6.5 | 6.5 | 1.5 | 6.5 | 0 | 130000 | 5.500000 | 2.333333 | 3.0 | 6 |  0.61446613 | -0.79801599 | 1 | 0 |       NA | 1 | 0 |      NA | 1 | 0 |    NA | 1 | 0 |      NA | 0 | 0 |       NA | 1 | 0 |     NA | 1 | 0 |     NA | 0 | 0 |     NA | 0 | 0 |      NA | 0 | 1 | 5.0 | 0 | 0 |       NA | 0 | 0 |    NA | 0 | 0 |     NA |\n",
       "| 4 | 4 | 1097 | male   | east_asian     | Otero     | 6.000000 | 5.000 | 2.000000 | 2.0 | 3.0 | 4.0 | 5.5 | 1.5 | 4.5 | 0 |  90000 | 5.833333 | 2.000000 | 2.5 | 6 |  0.22618824 | -0.04126834 | 6 | 4 | 5.812500 | 3 | 1 | 5.62500 | 2 | 0 |    NA | 2 | 1 | 5.62500 | 1 | 1 | 5.625000 | 1 | 1 | 6.6250 | 0 | 1 | 5.6250 | 0 | 1 | 4.8750 | 1 | 1 | 6.12500 | 0 | 0 |  NA | 0 | 1 | 5.625000 | 0 | 1 | 6.000 | 3 | 1 | 6.6250 |\n",
       "| 5 | 5 | 1105 | female | white          | Larkin    | 6.666667 | 5.750 | 2.000000 | 2.0 | 4.5 | 5.0 | 5.0 | 2.0 | 4.5 | 0 | 170000 | 6.666667 | 2.000000 | 2.0 | 4 |  0.48041399 |  0.64021289 | 6 | 9 | 5.847222 | 4 | 5 | 6.02500 | 2 | 2 | 5.875 | 3 | 4 | 5.81250 | 2 | 3 | 6.125000 | 2 | 2 | 5.3750 | 2 | 2 | 6.3125 | 1 | 4 | 6.1875 | 2 | 1 | 6.87500 | 1 | 0 |  NA | 2 | 3 | 5.750000 | 1 | 1 | 5.125 | 4 | 2 | 6.1875 |\n",
       "| 6 | 6 | 1110 | female | white          | Larkin    | 5.166667 | 6.750 | 2.000000 | 2.0 | 6.5 | 5.0 | 6.0 | 2.0 | 4.5 | 0 | 110000 | 6.833333 | 1.666667 | 2.0 | 5 | -0.09159395 |  0.95635646 | 4 | 5 | 5.550000 | 4 | 4 | 5.40625 | 1 | 0 |    NA | 3 | 4 | 5.40625 | 3 | 4 | 5.406250 | 3 | 2 | 6.0625 | 2 | 2 | 5.7500 | 3 | 1 | 6.0000 | 2 | 4 | 5.78125 | 0 | 0 |  NA | 3 | 4 | 5.406250 | 4 | 1 | 6.000 | 3 | 4 | 5.4375 |\n",
       "\n"
      ],
      "text/plain": [
       "  X PID  gender race           dorm      life_satisfaction_t1 empathy\n",
       "1 1 1001 male   south_asian    Rinconada 4.833333             6.375  \n",
       "2 2 1047 female other_or_mixed Twain     5.333333             6.500  \n",
       "3 3 1078 female east_asian     Loro      5.500000             6.250  \n",
       "4 4 1097 male   east_asian     Otero     6.000000             5.000  \n",
       "5 5 1105 female white          Larkin    6.666667             5.750  \n",
       "6 6 1110 female white          Larkin    5.166667             6.750  \n",
       "  loneliness_t1 stress_t1 BFI_E BFI_A BFI_C BFI_N BFI_O intl_student\n",
       "1 1.333333      2.5       6.5   6.0   5.5   3.5   6.5   0           \n",
       "2 2.000000      2.5       5.5   6.5   7.0   3.0   6.5   0           \n",
       "3 1.333333      2.0       4.5   6.5   6.5   1.5   6.5   0           \n",
       "4 2.000000      2.0       3.0   4.0   5.5   1.5   4.5   0           \n",
       "5 2.000000      2.0       4.5   5.0   5.0   2.0   4.5   0           \n",
       "6 2.000000      2.0       6.5   5.0   6.0   2.0   4.5   0           \n",
       "  family_income life_satisfaction_t2 loneliness_t2 stress_t2\n",
       "1  70000        6.166667             2.000000      1.5      \n",
       "2 210000        5.166667             2.000000      3.0      \n",
       "3 130000        5.500000             2.333333      3.0      \n",
       "4  90000        5.833333             2.000000      2.5      \n",
       "5 170000        6.666667             2.000000      2.0      \n",
       "6 110000        6.833333             1.666667      2.0      \n",
       "  parent_education_highest wellbeing_composite_t1 wellbeing_composite_t2\n",
       "1 5                        -0.05862651             0.83993983           \n",
       "2 5                        -0.44690440            -0.66253028           \n",
       "3 6                         0.61446613            -0.79801599           \n",
       "4 6                         0.22618824            -0.04126834           \n",
       "5 4                         0.48041399             0.64021289           \n",
       "6 5                        -0.09159395             0.95635646           \n",
       "  degree_in_UNION degree_out_UNION empathy_UNION degree_in_INTIMATE\n",
       "1 3               0                      NA      1                 \n",
       "2 6               4                6.437500      4                 \n",
       "3 1               0                      NA      1                 \n",
       "4 6               4                5.812500      3                 \n",
       "5 6               9                5.847222      4                 \n",
       "6 4               5                5.550000      4                 \n",
       "  degree_out_INTIMATE empathy_INTIMATE degree_in_ACQUAINTANCE\n",
       "1 0                        NA          1                     \n",
       "2 4                   6.43750          0                     \n",
       "3 0                        NA          1                     \n",
       "4 1                   5.62500          2                     \n",
       "5 5                   6.02500          2                     \n",
       "6 4                   5.40625          1                     \n",
       "  degree_out_ACQUAINTANCE empathy_ACQUAINTANCE degree_in_CloseFrds\n",
       "1 0                          NA                1                  \n",
       "2 1                       6.125                4                  \n",
       "3 0                          NA                1                  \n",
       "4 0                          NA                2                  \n",
       "5 2                       5.875                3                  \n",
       "6 0                          NA                3                  \n",
       "  degree_out_CloseFrds empathy_CloseFrds degree_in_NegEmoSupp\n",
       "1 0                         NA           0                   \n",
       "2 4                    6.43750           4                   \n",
       "3 0                         NA           0                   \n",
       "4 1                    5.62500           1                   \n",
       "5 4                    5.81250           2                   \n",
       "6 4                    5.40625           3                   \n",
       "  degree_out_NegEmoSupp empathy_NegEmoSupp degree_in_PosEmoSupp\n",
       "1 0                           NA           2                   \n",
       "2 3                     6.541667           3                   \n",
       "3 0                           NA           1                   \n",
       "4 1                     5.625000           1                   \n",
       "5 3                     6.125000           2                   \n",
       "6 4                     5.406250           3                   \n",
       "  degree_out_PosEmoSupp empathy_PosEmoSupp degree_in_Responsive\n",
       "1 0                         NA             0                   \n",
       "2 4                     6.4375             2                   \n",
       "3 0                         NA             1                   \n",
       "4 1                     6.6250             0                   \n",
       "5 2                     5.3750             2                   \n",
       "6 2                     6.0625             2                   \n",
       "  degree_out_Responsive empathy_Responsive degree_in_EmpSupp degree_out_EmpSupp\n",
       "1 0                         NA             0                 0                 \n",
       "2 2                     6.5625             1                 2                 \n",
       "3 0                         NA             0                 0                 \n",
       "4 1                     5.6250             0                 1                 \n",
       "5 2                     6.3125             1                 4                 \n",
       "6 2                     5.7500             3                 1                 \n",
       "  empathy_EmpSupp degree_in_PosAff degree_out_PosAff empathy_PosAff\n",
       "1     NA          1                0                      NA       \n",
       "2 6.5625          3                4                 6.43750       \n",
       "3     NA          0                0                      NA       \n",
       "4 4.8750          1                1                 6.12500       \n",
       "5 6.1875          2                1                 6.87500       \n",
       "6 6.0000          2                4                 5.78125       \n",
       "  degree_in_NegAff degree_out_NegAff empathy_NegAff degree_in_Gossip\n",
       "1 0                0                  NA            1               \n",
       "2 1                1                 6.5            4               \n",
       "3 0                1                 5.0            0               \n",
       "4 0                0                  NA            0               \n",
       "5 1                0                  NA            2               \n",
       "6 0                0                  NA            3               \n",
       "  degree_out_Gossip empathy_Gossip degree_in_Liked degree_out_Liked\n",
       "1 0                       NA       1               0               \n",
       "2 3                 6.541667       1               2               \n",
       "3 0                       NA       0               0               \n",
       "4 1                 5.625000       0               1               \n",
       "5 3                 5.750000       1               1               \n",
       "6 4                 5.406250       4               1               \n",
       "  empathy_Liked degree_in_StudyWith degree_out_StudyWith empathy_StudyWith\n",
       "1    NA         1                   0                        NA           \n",
       "2 6.000         2                   1                    6.6250           \n",
       "3    NA         0                   0                        NA           \n",
       "4 6.000         3                   1                    6.6250           \n",
       "5 5.125         4                   2                    6.1875           \n",
       "6 6.000         3                   4                    5.4375           "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = read.csv(DATA_FILE, na.strings=c(\"\", \" \", \"NA\"))\n",
    "dim(df)\n",
    "head(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the base model predicting t2 from t1 wellbeing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A matrix: 2 × 4 of type dbl</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>Estimate</th><th scope=col>Std. Error</th><th scope=col>t value</th><th scope=col>Pr(&gt;|t|)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>(Intercept)</th><td>-0.01387322</td><td>0.03136429</td><td>-0.4423255</td><td>6.584260e-01</td></tr>\n",
       "\t<tr><th scope=row>wellbeing_composite_t1</th><td> 0.66765669</td><td>0.03156521</td><td>21.1516612</td><td>2.864762e-73</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 2 × 4 of type dbl\n",
       "\\begin{tabular}{r|llll}\n",
       "  & Estimate & Std. Error & t value & Pr(>\\textbar{}t\\textbar{})\\\\\n",
       "\\hline\n",
       "\t(Intercept) & -0.01387322 & 0.03136429 & -0.4423255 & 6.584260e-01\\\\\n",
       "\twellbeing\\_composite\\_t1 &  0.66765669 & 0.03156521 & 21.1516612 & 2.864762e-73\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 2 × 4 of type dbl\n",
       "\n",
       "| <!--/--> | Estimate | Std. Error | t value | Pr(&gt;|t|) |\n",
       "|---|---|---|---|---|\n",
       "| (Intercept) | -0.01387322 | 0.03136429 | -0.4423255 | 6.584260e-01 |\n",
       "| wellbeing_composite_t1 |  0.66765669 | 0.03156521 | 21.1516612 | 2.864762e-73 |\n",
       "\n"
      ],
      "text/plain": [
       "                       Estimate    Std. Error t value    Pr(>|t|)    \n",
       "(Intercept)            -0.01387322 0.03136429 -0.4423255 6.584260e-01\n",
       "wellbeing_composite_t1  0.66765669 0.03156521 21.1516612 2.864762e-73"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = lm(wellbeing_composite_t2 ~ wellbeing_composite_t1, df)\n",
    "summary(base_model)$coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can any network variable predict t2 wellbeing outright?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.frame: 40 × 4</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>column</th><th scope=col>beta</th><th scope=col>p</th><th scope=col>p_adjusted</th></tr>\n",
       "\t<tr><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>degree_in_PosAff       </td><td> 0.074478432</td><td>0.003853911</td><td>0.1315914</td></tr>\n",
       "\t<tr><td>degree_in_Liked        </td><td> 0.058850126</td><td>0.006579572</td><td>0.1315914</td></tr>\n",
       "\t<tr><td>degree_in_INTIMATE     </td><td> 0.041807682</td><td>0.039158391</td><td>0.2728465</td></tr>\n",
       "\t<tr><td>degree_in_CloseFrds    </td><td> 0.050651489</td><td>0.022976539</td><td>0.2728465</td></tr>\n",
       "\t<tr><td>degree_out_Responsive  </td><td> 0.075830152</td><td>0.032280957</td><td>0.2728465</td></tr>\n",
       "\t<tr><td>degree_out_NegAff      </td><td>-0.130562621</td><td>0.047098481</td><td>0.2728465</td></tr>\n",
       "\t<tr><td>degree_in_StudyWith    </td><td> 0.059552474</td><td>0.047748133</td><td>0.2728465</td></tr>\n",
       "\t<tr><td>empathy_Gossip         </td><td> 0.171418280</td><td>0.057667186</td><td>0.2883359</td></tr>\n",
       "\t<tr><td>degree_in_UNION        </td><td> 0.025867299</td><td>0.080796715</td><td>0.3073347</td></tr>\n",
       "\t<tr><td>degree_in_NegEmoSupp   </td><td> 0.046411682</td><td>0.084517051</td><td>0.3073347</td></tr>\n",
       "\t<tr><td>degree_in_PosEmoSupp   </td><td> 0.051485511</td><td>0.078043720</td><td>0.3073347</td></tr>\n",
       "\t<tr><td>degree_in_Responsive   </td><td> 0.047592600</td><td>0.103617022</td><td>0.3453901</td></tr>\n",
       "\t<tr><td>degree_out_NegEmoSupp  </td><td> 0.048359197</td><td>0.138831920</td><td>0.4021059</td></tr>\n",
       "\t<tr><td>degree_out_PosAff      </td><td> 0.048643617</td><td>0.160842349</td><td>0.4021059</td></tr>\n",
       "\t<tr><td>empathy_Liked          </td><td>-0.144396299</td><td>0.144022316</td><td>0.4021059</td></tr>\n",
       "\t<tr><td>degree_out_StudyWith   </td><td> 0.050311378</td><td>0.159670864</td><td>0.4021059</td></tr>\n",
       "\t<tr><td>degree_in_ACQUAINTANCE </td><td> 0.048247005</td><td>0.178731884</td><td>0.4205456</td></tr>\n",
       "\t<tr><td>degree_out_ACQUAINTANCE</td><td>-0.040399031</td><td>0.358903274</td><td>0.7178065</td></tr>\n",
       "\t<tr><td>degree_out_Gossip      </td><td> 0.033228772</td><td>0.346880179</td><td>0.7178065</td></tr>\n",
       "\t<tr><td>degree_out_Liked       </td><td> 0.038353163</td><td>0.332883941</td><td>0.7178065</td></tr>\n",
       "\t<tr><td>degree_out_INTIMATE    </td><td> 0.019748171</td><td>0.487533593</td><td>0.7249046</td></tr>\n",
       "\t<tr><td>degree_out_CloseFrds   </td><td> 0.021356555</td><td>0.489310624</td><td>0.7249046</td></tr>\n",
       "\t<tr><td>degree_out_PosEmoSupp  </td><td> 0.023975314</td><td>0.471673093</td><td>0.7249046</td></tr>\n",
       "\t<tr><td>empathy_PosEmoSupp     </td><td> 0.065622267</td><td>0.463887689</td><td>0.7249046</td></tr>\n",
       "\t<tr><td>degree_out_EmpSupp     </td><td> 0.034165983</td><td>0.414579055</td><td>0.7249046</td></tr>\n",
       "\t<tr><td>empathy_EmpSupp        </td><td> 0.071515842</td><td>0.444199632</td><td>0.7249046</td></tr>\n",
       "\t<tr><td>degree_in_NegAff       </td><td>-0.034222319</td><td>0.452782493</td><td>0.7249046</td></tr>\n",
       "\t<tr><td>empathy_UNION          </td><td>-0.047447755</td><td>0.638867384</td><td>0.7743847</td></tr>\n",
       "\t<tr><td>empathy_INTIMATE       </td><td>-0.045854993</td><td>0.634039423</td><td>0.7743847</td></tr>\n",
       "\t<tr><td>empathy_ACQUAINTANCE   </td><td>-0.053323783</td><td>0.619873831</td><td>0.7743847</td></tr>\n",
       "\t<tr><td>degree_in_EmpSupp      </td><td>-0.015246958</td><td>0.601346714</td><td>0.7743847</td></tr>\n",
       "\t<tr><td>degree_in_Gossip       </td><td> 0.017117553</td><td>0.585016964</td><td>0.7743847</td></tr>\n",
       "\t<tr><td>empathy_StudyWith      </td><td> 0.054009220</td><td>0.570295139</td><td>0.7743847</td></tr>\n",
       "\t<tr><td>empathy_CloseFrds      </td><td>-0.036523593</td><td>0.686927487</td><td>0.8081500</td></tr>\n",
       "\t<tr><td>degree_out_UNION       </td><td> 0.006154255</td><td>0.748330942</td><td>0.8552354</td></tr>\n",
       "\t<tr><td>empathy                </td><td> 0.014044116</td><td>0.822527003</td><td>0.8717175</td></tr>\n",
       "\t<tr><td>empathy_NegEmoSupp     </td><td> 0.023942834</td><td>0.791770489</td><td>0.8717175</td></tr>\n",
       "\t<tr><td>empathy_NegAff         </td><td> 0.030612556</td><td>0.828131664</td><td>0.8717175</td></tr>\n",
       "\t<tr><td>empathy_Responsive     </td><td> 0.013588703</td><td>0.880375434</td><td>0.9029492</td></tr>\n",
       "\t<tr><td>empathy_PosAff         </td><td> 0.003132939</td><td>0.972006727</td><td>0.9720067</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 40 × 4\n",
       "\\begin{tabular}{llll}\n",
       " column & beta & p & p\\_adjusted\\\\\n",
       " <fct> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t degree\\_in\\_PosAff        &  0.074478432 & 0.003853911 & 0.1315914\\\\\n",
       "\t degree\\_in\\_Liked         &  0.058850126 & 0.006579572 & 0.1315914\\\\\n",
       "\t degree\\_in\\_INTIMATE      &  0.041807682 & 0.039158391 & 0.2728465\\\\\n",
       "\t degree\\_in\\_CloseFrds     &  0.050651489 & 0.022976539 & 0.2728465\\\\\n",
       "\t degree\\_out\\_Responsive   &  0.075830152 & 0.032280957 & 0.2728465\\\\\n",
       "\t degree\\_out\\_NegAff       & -0.130562621 & 0.047098481 & 0.2728465\\\\\n",
       "\t degree\\_in\\_StudyWith     &  0.059552474 & 0.047748133 & 0.2728465\\\\\n",
       "\t empathy\\_Gossip          &  0.171418280 & 0.057667186 & 0.2883359\\\\\n",
       "\t degree\\_in\\_UNION         &  0.025867299 & 0.080796715 & 0.3073347\\\\\n",
       "\t degree\\_in\\_NegEmoSupp    &  0.046411682 & 0.084517051 & 0.3073347\\\\\n",
       "\t degree\\_in\\_PosEmoSupp    &  0.051485511 & 0.078043720 & 0.3073347\\\\\n",
       "\t degree\\_in\\_Responsive    &  0.047592600 & 0.103617022 & 0.3453901\\\\\n",
       "\t degree\\_out\\_NegEmoSupp   &  0.048359197 & 0.138831920 & 0.4021059\\\\\n",
       "\t degree\\_out\\_PosAff       &  0.048643617 & 0.160842349 & 0.4021059\\\\\n",
       "\t empathy\\_Liked           & -0.144396299 & 0.144022316 & 0.4021059\\\\\n",
       "\t degree\\_out\\_StudyWith    &  0.050311378 & 0.159670864 & 0.4021059\\\\\n",
       "\t degree\\_in\\_ACQUAINTANCE  &  0.048247005 & 0.178731884 & 0.4205456\\\\\n",
       "\t degree\\_out\\_ACQUAINTANCE & -0.040399031 & 0.358903274 & 0.7178065\\\\\n",
       "\t degree\\_out\\_Gossip       &  0.033228772 & 0.346880179 & 0.7178065\\\\\n",
       "\t degree\\_out\\_Liked        &  0.038353163 & 0.332883941 & 0.7178065\\\\\n",
       "\t degree\\_out\\_INTIMATE     &  0.019748171 & 0.487533593 & 0.7249046\\\\\n",
       "\t degree\\_out\\_CloseFrds    &  0.021356555 & 0.489310624 & 0.7249046\\\\\n",
       "\t degree\\_out\\_PosEmoSupp   &  0.023975314 & 0.471673093 & 0.7249046\\\\\n",
       "\t empathy\\_PosEmoSupp      &  0.065622267 & 0.463887689 & 0.7249046\\\\\n",
       "\t degree\\_out\\_EmpSupp      &  0.034165983 & 0.414579055 & 0.7249046\\\\\n",
       "\t empathy\\_EmpSupp         &  0.071515842 & 0.444199632 & 0.7249046\\\\\n",
       "\t degree\\_in\\_NegAff        & -0.034222319 & 0.452782493 & 0.7249046\\\\\n",
       "\t empathy\\_UNION           & -0.047447755 & 0.638867384 & 0.7743847\\\\\n",
       "\t empathy\\_INTIMATE        & -0.045854993 & 0.634039423 & 0.7743847\\\\\n",
       "\t empathy\\_ACQUAINTANCE    & -0.053323783 & 0.619873831 & 0.7743847\\\\\n",
       "\t degree\\_in\\_EmpSupp       & -0.015246958 & 0.601346714 & 0.7743847\\\\\n",
       "\t degree\\_in\\_Gossip        &  0.017117553 & 0.585016964 & 0.7743847\\\\\n",
       "\t empathy\\_StudyWith       &  0.054009220 & 0.570295139 & 0.7743847\\\\\n",
       "\t empathy\\_CloseFrds       & -0.036523593 & 0.686927487 & 0.8081500\\\\\n",
       "\t degree\\_out\\_UNION        &  0.006154255 & 0.748330942 & 0.8552354\\\\\n",
       "\t empathy                 &  0.014044116 & 0.822527003 & 0.8717175\\\\\n",
       "\t empathy\\_NegEmoSupp      &  0.023942834 & 0.791770489 & 0.8717175\\\\\n",
       "\t empathy\\_NegAff          &  0.030612556 & 0.828131664 & 0.8717175\\\\\n",
       "\t empathy\\_Responsive      &  0.013588703 & 0.880375434 & 0.9029492\\\\\n",
       "\t empathy\\_PosAff          &  0.003132939 & 0.972006727 & 0.9720067\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 40 × 4\n",
       "\n",
       "| column &lt;fct&gt; | beta &lt;dbl&gt; | p &lt;dbl&gt; | p_adjusted &lt;dbl&gt; |\n",
       "|---|---|---|---|\n",
       "| degree_in_PosAff        |  0.074478432 | 0.003853911 | 0.1315914 |\n",
       "| degree_in_Liked         |  0.058850126 | 0.006579572 | 0.1315914 |\n",
       "| degree_in_INTIMATE      |  0.041807682 | 0.039158391 | 0.2728465 |\n",
       "| degree_in_CloseFrds     |  0.050651489 | 0.022976539 | 0.2728465 |\n",
       "| degree_out_Responsive   |  0.075830152 | 0.032280957 | 0.2728465 |\n",
       "| degree_out_NegAff       | -0.130562621 | 0.047098481 | 0.2728465 |\n",
       "| degree_in_StudyWith     |  0.059552474 | 0.047748133 | 0.2728465 |\n",
       "| empathy_Gossip          |  0.171418280 | 0.057667186 | 0.2883359 |\n",
       "| degree_in_UNION         |  0.025867299 | 0.080796715 | 0.3073347 |\n",
       "| degree_in_NegEmoSupp    |  0.046411682 | 0.084517051 | 0.3073347 |\n",
       "| degree_in_PosEmoSupp    |  0.051485511 | 0.078043720 | 0.3073347 |\n",
       "| degree_in_Responsive    |  0.047592600 | 0.103617022 | 0.3453901 |\n",
       "| degree_out_NegEmoSupp   |  0.048359197 | 0.138831920 | 0.4021059 |\n",
       "| degree_out_PosAff       |  0.048643617 | 0.160842349 | 0.4021059 |\n",
       "| empathy_Liked           | -0.144396299 | 0.144022316 | 0.4021059 |\n",
       "| degree_out_StudyWith    |  0.050311378 | 0.159670864 | 0.4021059 |\n",
       "| degree_in_ACQUAINTANCE  |  0.048247005 | 0.178731884 | 0.4205456 |\n",
       "| degree_out_ACQUAINTANCE | -0.040399031 | 0.358903274 | 0.7178065 |\n",
       "| degree_out_Gossip       |  0.033228772 | 0.346880179 | 0.7178065 |\n",
       "| degree_out_Liked        |  0.038353163 | 0.332883941 | 0.7178065 |\n",
       "| degree_out_INTIMATE     |  0.019748171 | 0.487533593 | 0.7249046 |\n",
       "| degree_out_CloseFrds    |  0.021356555 | 0.489310624 | 0.7249046 |\n",
       "| degree_out_PosEmoSupp   |  0.023975314 | 0.471673093 | 0.7249046 |\n",
       "| empathy_PosEmoSupp      |  0.065622267 | 0.463887689 | 0.7249046 |\n",
       "| degree_out_EmpSupp      |  0.034165983 | 0.414579055 | 0.7249046 |\n",
       "| empathy_EmpSupp         |  0.071515842 | 0.444199632 | 0.7249046 |\n",
       "| degree_in_NegAff        | -0.034222319 | 0.452782493 | 0.7249046 |\n",
       "| empathy_UNION           | -0.047447755 | 0.638867384 | 0.7743847 |\n",
       "| empathy_INTIMATE        | -0.045854993 | 0.634039423 | 0.7743847 |\n",
       "| empathy_ACQUAINTANCE    | -0.053323783 | 0.619873831 | 0.7743847 |\n",
       "| degree_in_EmpSupp       | -0.015246958 | 0.601346714 | 0.7743847 |\n",
       "| degree_in_Gossip        |  0.017117553 | 0.585016964 | 0.7743847 |\n",
       "| empathy_StudyWith       |  0.054009220 | 0.570295139 | 0.7743847 |\n",
       "| empathy_CloseFrds       | -0.036523593 | 0.686927487 | 0.8081500 |\n",
       "| degree_out_UNION        |  0.006154255 | 0.748330942 | 0.8552354 |\n",
       "| empathy                 |  0.014044116 | 0.822527003 | 0.8717175 |\n",
       "| empathy_NegEmoSupp      |  0.023942834 | 0.791770489 | 0.8717175 |\n",
       "| empathy_NegAff          |  0.030612556 | 0.828131664 | 0.8717175 |\n",
       "| empathy_Responsive      |  0.013588703 | 0.880375434 | 0.9029492 |\n",
       "| empathy_PosAff          |  0.003132939 | 0.972006727 | 0.9720067 |\n",
       "\n"
      ],
      "text/plain": [
       "   column                  beta         p           p_adjusted\n",
       "1  degree_in_PosAff         0.074478432 0.003853911 0.1315914 \n",
       "2  degree_in_Liked          0.058850126 0.006579572 0.1315914 \n",
       "3  degree_in_INTIMATE       0.041807682 0.039158391 0.2728465 \n",
       "4  degree_in_CloseFrds      0.050651489 0.022976539 0.2728465 \n",
       "5  degree_out_Responsive    0.075830152 0.032280957 0.2728465 \n",
       "6  degree_out_NegAff       -0.130562621 0.047098481 0.2728465 \n",
       "7  degree_in_StudyWith      0.059552474 0.047748133 0.2728465 \n",
       "8  empathy_Gossip           0.171418280 0.057667186 0.2883359 \n",
       "9  degree_in_UNION          0.025867299 0.080796715 0.3073347 \n",
       "10 degree_in_NegEmoSupp     0.046411682 0.084517051 0.3073347 \n",
       "11 degree_in_PosEmoSupp     0.051485511 0.078043720 0.3073347 \n",
       "12 degree_in_Responsive     0.047592600 0.103617022 0.3453901 \n",
       "13 degree_out_NegEmoSupp    0.048359197 0.138831920 0.4021059 \n",
       "14 degree_out_PosAff        0.048643617 0.160842349 0.4021059 \n",
       "15 empathy_Liked           -0.144396299 0.144022316 0.4021059 \n",
       "16 degree_out_StudyWith     0.050311378 0.159670864 0.4021059 \n",
       "17 degree_in_ACQUAINTANCE   0.048247005 0.178731884 0.4205456 \n",
       "18 degree_out_ACQUAINTANCE -0.040399031 0.358903274 0.7178065 \n",
       "19 degree_out_Gossip        0.033228772 0.346880179 0.7178065 \n",
       "20 degree_out_Liked         0.038353163 0.332883941 0.7178065 \n",
       "21 degree_out_INTIMATE      0.019748171 0.487533593 0.7249046 \n",
       "22 degree_out_CloseFrds     0.021356555 0.489310624 0.7249046 \n",
       "23 degree_out_PosEmoSupp    0.023975314 0.471673093 0.7249046 \n",
       "24 empathy_PosEmoSupp       0.065622267 0.463887689 0.7249046 \n",
       "25 degree_out_EmpSupp       0.034165983 0.414579055 0.7249046 \n",
       "26 empathy_EmpSupp          0.071515842 0.444199632 0.7249046 \n",
       "27 degree_in_NegAff        -0.034222319 0.452782493 0.7249046 \n",
       "28 empathy_UNION           -0.047447755 0.638867384 0.7743847 \n",
       "29 empathy_INTIMATE        -0.045854993 0.634039423 0.7743847 \n",
       "30 empathy_ACQUAINTANCE    -0.053323783 0.619873831 0.7743847 \n",
       "31 degree_in_EmpSupp       -0.015246958 0.601346714 0.7743847 \n",
       "32 degree_in_Gossip         0.017117553 0.585016964 0.7743847 \n",
       "33 empathy_StudyWith        0.054009220 0.570295139 0.7743847 \n",
       "34 empathy_CloseFrds       -0.036523593 0.686927487 0.8081500 \n",
       "35 degree_out_UNION         0.006154255 0.748330942 0.8552354 \n",
       "36 empathy                  0.014044116 0.822527003 0.8717175 \n",
       "37 empathy_NegEmoSupp       0.023942834 0.791770489 0.8717175 \n",
       "38 empathy_NegAff           0.030612556 0.828131664 0.8717175 \n",
       "39 empathy_Responsive       0.013588703 0.880375434 0.9029492 \n",
       "40 empathy_PosAff           0.003132939 0.972006727 0.9720067 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "frame = NULL\n",
    "for (col in names(df)) {\n",
    "    if (startsWith(col, 'degree') | startsWith(col, 'empathy')) {\n",
    "        model = lm(as.formula(paste('wellbeing_composite_t2 ~', col)), df)\n",
    "        b = summary(model)$coefficients[2, 1]\n",
    "        p = summary(model)$coefficients[2, 4]\n",
    "        frame = rbind(frame, c(col, b, p))\n",
    "    }\n",
    "}\n",
    "frame = as.data.frame(frame)\n",
    "names(frame) = c('column', 'beta', 'p')\n",
    "frame$beta = as.numeric(as.character(frame$beta))\n",
    "frame$p = as.numeric(as.character(frame$p))\n",
    "frame$p_adjusted = p.adjust(frame$p, method=\"BH\")\n",
    "frame %>% arrange(p_adjusted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "r-3.6.2"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
